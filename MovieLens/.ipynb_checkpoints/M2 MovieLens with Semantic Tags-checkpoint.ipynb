{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhMT2zYloMpm"
   },
   "source": [
    "#**Proyecto - Sistemas Recomendadores - IIC3633**\n",
    "\n",
    "## Session-Based RNNs for Recommendation with content-aware attention\n",
    "\n",
    "### The main difference with respect to the baseline model is that,\n",
    "### instead of receiving a simple ItemId token, the session is made up of\n",
    "### the average GloVe encoding of the tags that the item has on the db\n",
    "\n",
    "### The model then tries to predict the closest average GloVe embedding, \n",
    "### given the session, and we compare with the truth given by the label\n",
    "\n",
    "### At test time, we simply take the kNN of the predicted embedding, and \n",
    "### that is the recommended that the system outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3kJRW-qQ17_Q",
    "outputId": "1e336f48-43aa-4929-cefd-63e02dee3449"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import humanize\n",
    "import pyreclab\n",
    "import GPUtil as GPU\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import getsizeof\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import cosine_proximity, categorical_crossentropy\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers.core import Permute, Reshape, RepeatVector\n",
    "from keras.layers import Input, Dense, Dropout, CuDNNGRU, Embedding, concatenate, Lambda, multiply, merge, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfW00EfwSNQ6"
   },
   "outputs": [],
   "source": [
    "# Cargamos dataframes preprocesados de MovieLens20MM\n",
    "PATH_TO_TRAIN = './data/all_train.csv'\n",
    "PATH_TO_DEV = './data/dev.csv'\n",
    "PATH_TO_TEST = './data/test.csv'\n",
    "PATH_TO_TAGS = './data/tags.pickle'\n",
    "\n",
    "train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
    "dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'ItemId':np.int64})\n",
    "test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})\n",
    "\n",
    "with open(PATH_TO_TAGS, 'rb') as file:\n",
    "    tags = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtencion de GLoVE. Referencia: github.com/abisee/cs224n-win18-squad\n",
    "_PAD = b\"<pad>\" # padding\n",
    "_UNK = b\"<unk>\" # unknown\n",
    "_START_VOCAB = [_PAD, _UNK]\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "\n",
    "\n",
    "def get_glove(glove_path, glove_dim):\n",
    "    #print(\"Loading GLoVE vectors from file: %s\" % glove_path)\n",
    "    vocab_size = int(4e5)\n",
    "\n",
    "    emb_matrix = np.zeros((vocab_size + len(_START_VOCAB), glove_dim))\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "\n",
    "    random_init = True\n",
    "    if random_init:\n",
    "        emb_matrix[:len(_START_VOCAB), :] = np.random.randn(len(_START_VOCAB), glove_dim)\n",
    "\n",
    "    idx = 0\n",
    "    for word in _START_VOCAB:\n",
    "        word2id[word] = idx\n",
    "        id2word[idx] = word\n",
    "        idx += 1\n",
    "\n",
    "    with open(glove_path, 'r') as fh:\n",
    "        for line in tqdm(fh, total=vocab_size):\n",
    "            line = line.lstrip().rstrip().split(\" \")\n",
    "            word = line[0]\n",
    "            vector = list(map(float, line[1:]))\n",
    "            if glove_dim != len(vector):\n",
    "                raise Exception(\"You set --glove_path=%s but --embedding_size=%i. If you set --glove_path yourself then make sure that --embedding_size matches!\" % (glove_path, glove_dim))\n",
    "            emb_matrix[idx, :] = vector\n",
    "            word2id[word] = idx\n",
    "            id2word[idx] = word\n",
    "            idx += 1\n",
    "\n",
    "    final_vocab_size = vocab_size + len(_START_VOCAB)\n",
    "    assert len(word2id) == final_vocab_size\n",
    "    assert len(id2word) == final_vocab_size\n",
    "    assert idx == final_vocab_size\n",
    "    \n",
    "    # retorno\n",
    "    # emb_matrix: (400002, glove_dim), glove embeddings (PAD and UNK first two rows)\n",
    "    # word2id: dictionary mapping word (string) to word id (int)\n",
    "    # id2word: dictionary mapping word id (int) to word (string)\n",
    "    \n",
    "    return emb_matrix, word2id, id2word   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6941/400000 [00:00<00:05, 69403.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GLoVE vectors from file: ./data/glove.6B.50d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:04<00:00, 83579.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generacion de batches. Referencia: github.com/abisee/cs224n-win18-squad\n",
    "glove_dimn = 50\n",
    "emb_matrix, word2id, id2word = get_glove(\"./data/glove.6B.{}d.txt\".format(glove_dimn), glove_dimn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lots', 'comedy', 'witty', 'heroic', 'children', 'adventure', 'boy', 'mission', 'weekly', 'unlikely', 'top', 'life', 'video', 'light', 'rotten', 'buddy', 'voice', 'buzz', 'friendship', 'travel', 'stereoscopic', 'woody', 'bright', 'engaging', 'computer', 'star', 'good', 'heart', 'fun', 'family', 'animated', 'clever', 'reissue', 'acting', 'entertainment', 'want', 'movie', 'funny', 'warm', 'animation', 'action', 'feature', 'humorous', 'story', 'see', 'figure', 'rousing', 'almost', 'national', 'cute', 'buy', 'film', 'come', 'daring', 'favorite', 'soothing', 'registry', 'innovative', 'toy', 'every', 'watched', 'best', 'rated', 'fantasy', 'classic', 'time', 'fanciful', 'cartoon', 'first'}\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[16769.16761031, -3841.06081035, -2105.25180516, ...,\n",
       "        10177.61202515, -7660.82383032,  1448.41599331],\n",
       "       [ 2262.25710803, -7477.76616702,  2873.25372288, ...,\n",
       "        -1744.65777403,  4658.42713516, -5572.08397868],\n",
       "       [ 2660.50942029,  1589.17701449, -2624.99352899, ...,\n",
       "        -1171.83346739,  -732.84941304, -5001.56676449],\n",
       "       ...,\n",
       "       [-3257.59647464,   373.65518188,  6945.96634058, ...,\n",
       "        -1591.40471377, -7160.46195652, 10096.56960145],\n",
       "       [-4830.7977029 , -3018.59616667,  3015.03184783, ...,\n",
       "         5025.30767391,   -89.8462942 ,  4104.05855072],\n",
       "       [  462.1966808 , -3271.08996739,  3009.30347826, ...,\n",
       "        -1203.40314855, -3756.60111232,  3536.24982971]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_avg_embedding(tags, emb_matrix):\n",
    "    # considering all tags for a given movie\n",
    "    all_results = None # np.zeros((emb_matrix.shape[0], word2id[0].shape[1]))\n",
    "    counter = 1\n",
    "    \n",
    "    for tag in tags:\n",
    "        if type(all_results) == np.ndarray:\n",
    "            print(counter)\n",
    "            all_results = np.average([all_results, np.dot(emb_matrix, word2id[tag])], \n",
    "                                     axis=0, \n",
    "                                     weights=(1-1/counter, 1/counter))\n",
    "        else:\n",
    "            all_results = np.dot(emb_matrix, word2id[tag])\n",
    "        counter += 1                             \n",
    "        \n",
    "    #all_results = np.array(all_results)\n",
    "    #all_results = np.average(all_results)\n",
    "    return all_results\n",
    "\n",
    "print(tags[1])\n",
    "\n",
    "# To use in the batch_generator\n",
    "# for movie in tags_df:\n",
    "#     get_avg_embedding(movie, emb_matrix)\n",
    "\n",
    "get_avg_embedding(tags[1], emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19545"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFkivhjnvrKO"
   },
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size=128, session_max_len=19, fraction=1, offset=0, embedding=True, n_items=None, itemids=None, itemidmap=None, aug = True):\n",
    "    item_key = 'ItemId'\n",
    "    session_key = 'SessionId'\n",
    "    time_key = 'Time'\n",
    "    \n",
    "    # add column\n",
    "    data = pd.merge(data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') \n",
    "\n",
    "    # sort by session\n",
    "    data.sort_values([session_key, time_key], inplace=True) \n",
    "\n",
    "    length = len(data['ItemId'])\n",
    "    \n",
    "    # array with cummulative offset that gives each session start in the array\n",
    "    offset_sessions = np.zeros(data[session_key].nunique()+1, dtype=np.int32)\n",
    "    offset_sessions[1:] = data.groupby(session_key).size().cumsum() \n",
    "    \n",
    "    actual_session = 0 + offset\n",
    "    \n",
    "    batch_feats = None\n",
    "    batch_labels = None\n",
    "\n",
    "    while True:\n",
    "        # session info\n",
    "        datum = data[offset_sessions[actual_session]:offset_sessions[actual_session+1]][item_key]  \n",
    "        datum = datum.values.reshape(-1,1)          \n",
    "        \n",
    "        for i in range(offset_sessions[actual_session+1]-offset_sessions[actual_session]-1):\n",
    "            if not aug:\n",
    "                if (i != offset_sessions[actual_session+1]-offset_sessions[actual_session]-2):\n",
    "                    continue\n",
    "                    \n",
    "            feats = datum[0:i+1]\n",
    "\n",
    "            if feats.shape[0] > session_max_len:\n",
    "                # take newest events\n",
    "                feats = feats[feats.shape[0]-session_max_len:] \n",
    "            else:\n",
    "                # left pad with zeros\n",
    "                feats = np.append(np.zeros((session_max_len-feats.shape[0],1), dtype=np.int8), feats)\n",
    "\n",
    "            feats = feats.reshape(1,-1) \n",
    "            label = datum[i+1]\n",
    "            label = np.expand_dims(label, axis=0)  # Termina siendo (1, dimn_previa)\n",
    "\n",
    "            if not isinstance(batch_feats, type(feats)):\n",
    "                batch_feats = feats\n",
    "            else:\n",
    "                batch_feats = np.append(batch_feats, feats, axis=0)\n",
    "\n",
    "            if not isinstance(batch_labels, type(label)):\n",
    "                batch_labels = label\n",
    "            else:\n",
    "                batch_labels = np.append(batch_labels, label, axis=0)\n",
    "\n",
    "\n",
    "            if batch_labels.shape[0] == batch_size:\n",
    "                if not embedding:\n",
    "                    batch_labels = to_categorical(itemidmap[batch_labels.flatten()], num_classes=n_items)\n",
    "                    pass\n",
    "\n",
    "                yield batch_feats, batch_labels\n",
    "                \n",
    "                # resume batch generation\n",
    "                batch_feats = None\n",
    "                batch_labels = None\n",
    "\n",
    "        actual_session = (actual_session + 1) % len(offset_sessions)\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "eQslL5CpRjdQ",
    "outputId": "bd8d7ef2-638d-42a1-eaf5-6e96636a7be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items unicos training: 12811\n",
      "Items unicos dev: 10103\n",
      "Items unicos testing: 10365\n",
      "Sesiones training: 80466\n",
      "Sesiones validation: 5747\n",
      "Sesiones testing: 5270\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512 #como en el paper\n",
    "session_max_len = 100\n",
    "embeddingp=False\n",
    "\n",
    "n_items = len(train_data['ItemId'].unique())+1\n",
    "print(\"Items unicos training:\", n_items)\n",
    "\n",
    "dev_n_items = len(dev_data['ItemId'].unique())+1\n",
    "print(\"Items unicos dev:\", dev_n_items)\n",
    "\n",
    "test_n_items = len(test_data['ItemId'].unique())+1\n",
    "print(\"Items unicos testing:\", test_n_items)\n",
    "\n",
    "train_samples_qty = len(train_data['SessionId'].unique()) # cantidad sesiones no augmentadas de train\n",
    "print(\"Sesiones training:\", train_samples_qty)\n",
    "\n",
    "dev_samples_qty = len(dev_data['SessionId'].unique()) # cantidad sesiones no augmentadas de dev\n",
    "print(\"Sesiones validation:\",dev_samples_qty)\n",
    "\n",
    "test_samples_qty = len(test_data['SessionId'].unique()) # cantidad sesiones no augmentadas de test\n",
    "print(\"Sesiones testing:\", test_samples_qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1i_adI_ASgDi"
   },
   "outputs": [],
   "source": [
    "train_fraction = 1 #256 # 1/fraction es la cantidad de sesiones mas recientes a considerar\n",
    "dev_fraction = 1 #2\n",
    "\n",
    "train_offset_step=train_samples_qty//batch_size\n",
    "dev_offset_step=dev_samples_qty//batch_size\n",
    "test_offset_step=test_samples_qty//batch_size\n",
    "\n",
    "\n",
    "aux = [0]\n",
    "aux.extend(list(train_data['ItemId'].unique()))\n",
    "itemids = np.array(aux)\n",
    "itemidmap = pd.Series(data=np.arange(n_items), index=itemids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "O5_sa72xSF50",
    "outputId": "b1a69ec9-6e86-44ba-d19e-69fe1f59cf57"
   },
   "outputs": [],
   "source": [
    "# Modelo\n",
    "\n",
    "# ToDo: self-attention\n",
    "\n",
    "def attention_3d_block(inputs, TIME_STEPS, SINGLE_ATTENTION_VECTOR=True):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul\n",
    "    \n",
    "emb_size = 50\n",
    "size = emb_size\n",
    "#size = emb_size if embeddingp else n_items\n",
    "\n",
    "inputs = Input(shape=(session_max_len,))\n",
    "#emb = Embedding(n_items, emb_size, embeddings_initializer='uniform', input_length=session_max_len)(inputs)\n",
    "emb = Embedding(400002, glove_dimn, weights=[emb_matrix], trainable=False, input_length=session_max_len)(inputs)\n",
    "drop1 = Dropout(0.25)(emb)\n",
    "gru = CuDNNGRU(100)(drop1) # , return_sequences=True\n",
    "drop2 = Dropout(0.25)(gru)\n",
    "#attention_mul = attention_3d_block(drop2, session_max_len)\n",
    "#attention_mul = Flatten()(attention_mul)\n",
    "predictions = Dense(n_items, activation='softmax')(drop2)#(attention_mul)\n",
    "model = Model(input=inputs, output=[predictions])\n",
    "#custom_loss = custom_cosine_loss(itemidmap, n_items)\n",
    "# lr original es 0.0001\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# Try Nadam, too\n",
    "model.compile(loss=categorical_crossentropy, optimizer=opt)\n",
    "model.summary()\n",
    "\n",
    "#filepath='./bast/model_checkpoint'\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "callbacks_list = []#[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "1XFQrjF7TZlU",
    "outputId": "9c209770-d67a-4bb3-a0ac-ed2a2a65fc0e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_epoca = 1\n",
    "for epoch in range(50):\n",
    "    #filepath='./weights/model_{}'.format(real_epoca)\n",
    "    #model.load_weights('./weights/model_{}'.format(real_epoca-1))\n",
    "    #model.save_weights(filepath)\n",
    "    train_generator = batch_generator(train_data, \n",
    "                                      batch_size=batch_size, \n",
    "                                  session_max_len=session_max_len,\n",
    "                                      fraction=train_fraction, \n",
    "                                      offset=train_offset_step*epoch,\n",
    "                                     embedding=embeddingp,\n",
    "                                      n_items=n_items,\n",
    "                                     itemids=itemids,\n",
    "                                     itemidmap=itemidmap)\n",
    "    \n",
    "    dev_generator = batch_generator(dev_data, \n",
    "                                    batch_size=batch_size,\n",
    "                                  session_max_len=session_max_len,\n",
    "                                    fraction=dev_fraction, \n",
    "                                    offset=dev_offset_step*epoch,\n",
    "                                    embedding=embeddingp,\n",
    "                                    n_items=n_items,\n",
    "                                    itemids=itemids,\n",
    "                                     itemidmap=itemidmap)\n",
    "    \n",
    "    history = model.fit_generator(train_generator,\n",
    "                                steps_per_epoch=train_offset_step,\n",
    "                                epochs=1,\n",
    "                                validation_data=dev_generator,\n",
    "                                validation_steps=dev_offset_step,\n",
    "                                callbacks=callbacks_list)\n",
    "    \n",
    "    real_epoca += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[1].get_weights()[0]\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# RECALL @ 10\n",
    "recall_k = 20\n",
    "\n",
    "#nbrs = NearestNeighbors(n_neighbors=recall_k, algorithm='ball_tree').fit(weights)\n",
    "#distances, indices = nbrs.kneighbors(weights) # Vienen ya ordenados! # Shape (37484, 20)\n",
    "# Paso 3: Dado un vector embedding arbitrario, obtener el item más cercano a éste. Aplicarla sobre los 20 anteriores.\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "test_generator = batch_generator(test_data, \n",
    "                                  batch_size=batch_size,\n",
    "                                  session_max_len=session_max_len,\n",
    "                                  fraction=train_fraction, \n",
    "                                  offset=0,\n",
    "                                 embedding=embeddingp,\n",
    "                                  n_items=n_items,\n",
    "                                 itemids=itemids,\n",
    "                                 itemidmap=itemidmap)\n",
    "\n",
    "\n",
    "n = 0\n",
    "suma = 0\n",
    "suma_baseline = 0\n",
    "while True:\n",
    "    try:\n",
    "        test_batch = next(test_generator)\n",
    "        pred = model.predict(test_batch[0]) # batch_size, n_items => 512, 37484\n",
    "        \n",
    "\n",
    "        label = test_batch[1]               \n",
    "\n",
    "        if n%100 == 0:\n",
    "            print(n)\n",
    "        #print(pred.shape)\n",
    "        #print(label.shape) \n",
    "\n",
    "        for row_idx in range(test_batch[0].shape[0]):\n",
    "          #print(test_batch[0][row_idx])\n",
    "          #baseline_pred = obj.recommend( str(test_batch[0][row_idx][-1]), 20 )\n",
    "          pred_row = pred[row_idx] # 37484, #.reshape(1, -1) # 50,\n",
    "          label_row = label[row_idx]        #.reshape(1, -1) # 50,\n",
    "\n",
    "          #print(pred_row.shape)\n",
    "          #print(label_row.shape)\n",
    "\n",
    "          idx1 = pred_row.argsort()[-recall_k:][::-1]\n",
    "          idx2 = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "          n += 1\n",
    "          #print(idx1)\n",
    "          #print(idx2)\n",
    "          if idx2[0] in idx1:\n",
    "            suma += 1\n",
    "\n",
    "          #if idx2[0] in baseline_pred:\n",
    "          #  suma_baseline += 1\n",
    "\n",
    "    except:\n",
    "        break\n",
    "print(\"Recall@{} epoch {}: {}\".format(recall_k, epoch, suma/n))\n",
    "\n",
    "#print(\"Recall@{} baseline: {}\".format(recall_k, suma_baseline/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All train set\n",
    "Recall@10 epoch 29: 0.08087340943113773\n",
    "Recall@20: 0.10473194236526946\n",
    "\n",
    "vs Hidasi\n",
    "\n",
    "Recall @ 20 0.2177499329156604\n",
    "MRR@20: 0.06513681594077811\n",
    "\n",
    "\n",
    "\n",
    "# Train Set\n",
    "Recall@10 epoch ..100?: 0.09546921781437126\n",
    "\n",
    "Recall@10 epoch 14: 0.06404879908501715\n",
    "\n",
    "Recall @20 epoch 99: 0.08705440681137724\n",
    "\n",
    "Con session_max_len = 100:\n",
    "\n",
    "Recall @20 epoch 9: 0.12195335890718563\n",
    "\n",
    "Con dwell_time NO FUNCIONA BIEN. Hacer ese supuesto en este dataset no tiene sentido.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IIC3633_M1_Colab_V2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
