{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhMT2zYloMpm"
   },
   "source": [
    "#**Proyecto - Sistemas Recomendadores - IIC3633**\n",
    "\n",
    "## Implementación en Keras de Session-Based RNNs for Recommendation con soft atenttion\n",
    "\n",
    "### V2: Implementación de embedding sobre one-hot vectors para entrenamiento más eficiente y modelo más chico\n",
    "\n",
    "\n",
    "Preliminar: Configuración entorno GPUs, Google Drive, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3kJRW-qQ17_Q",
    "outputId": "1e336f48-43aa-4929-cefd-63e02dee3449"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import psutil\n",
    "import humanize\n",
    "import pyreclab\n",
    "import GPUtil as GPU\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import cosine_proximity, categorical_crossentropy\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input, Dense, Dropout, CuDNNGRU, Embedding, concatenate, Lambda, multiply\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfW00EfwSNQ6"
   },
   "outputs": [],
   "source": [
    "# Cargamos dataframes preprocesados de RSC15\n",
    "PATH_TO_TRAIN = './data/train.csv'\n",
    "PATH_TO_DEV = './data/dev.csv'\n",
    "PATH_TO_TEST = './data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
    "dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'ItemId':np.int64})\n",
    "test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    def __init__(self, data, sep='\\t', session_key='SessionId', item_key='ItemId', time_key='Time', n_samples=-1, itemmap=None, time_sort=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: path of the csv file\n",
    "            sep: separator for the csv\n",
    "            session_key, item_key, time_key: name of the fields corresponding to the sessions, items, time\n",
    "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
    "            itemmap: mapping between item IDs and item indices\n",
    "            time_sort: whether to sort the sessions by time or not\n",
    "        \"\"\"\n",
    "        #self.df = pd.read_csv(path, sep=sep, names=[session_key, item_key, time_key])\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        self.df.sort_values([session_key, time_key], inplace=True) \n",
    "        # sampling\n",
    "        #if n_samples > 0: self.df = self.df[:n_samples] \n",
    "        # Add item indices\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "        \"\"\"\n",
    "        Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "        \"\"\"\n",
    "\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "        \n",
    "        \n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "\n",
    "        return offsets\n",
    "    \n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\" Order the session indices \"\"\"\n",
    "        if self.time_sort:\n",
    "            # starting time for each sessions, sorted by session IDs\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            # order the session indices by session starting times\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "\n",
    "        return session_idx_arr\n",
    "    \n",
    "    \n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\" \n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # unique item ids\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            itemmap = pd.DataFrame({self.item_key:item_ids,\n",
    "                                   'item_idx':item2idx[item_ids].values})\n",
    "        \n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "        \n",
    "    \n",
    "    @property    \n",
    "    def items(self):\n",
    "        return self.itemmap.ItemId.unique()\n",
    "        \n",
    "\n",
    "class SessionDataLoader:\n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "             dataset (SessionDataset): the session dataset to generate the batches from\n",
    "             batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        # initializations\n",
    "        df = self.dataset.df\n",
    "        session_key='SessionId'\n",
    "        item_key='ItemId'\n",
    "        time_key='TimeStamp'\n",
    "        self.n_items = df[item_key].nunique()\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = [] # indicator for the sessions to be terminated\n",
    "        finished = False        \n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices(for embedding) for clicks where the first sessions start\n",
    "            idx_target = df.item_idx.values[start]\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                input = idx_input\n",
    "                target = idx_target\n",
    "                #print(input)\n",
    "                #print(target)\n",
    "                yield input, target, mask\n",
    "                \n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            for idx in mask:\n",
    "                #print(idx)\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFkivhjnvrKO"
   },
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size=128, session_max_len=19, fraction=1, offset=0, embedding=True, n_items=None, itemids=None, itemidmap=None, aug = True):\n",
    "    item_key = 'ItemId'\n",
    "    session_key = 'SessionId'\n",
    "    time_key = 'Time'\n",
    "    \n",
    "    data = pd.merge(data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\n",
    "    \n",
    "    #print(\"Cantidad de samples: {}\".format(len(data)//fraction))\n",
    "\n",
    "    data.sort_values([session_key, time_key], inplace=True) # ordenamos por sesion\n",
    "\n",
    "    #data.sort_values([time_key], inplace=True)\n",
    "    length = len(data['ItemId'])\n",
    "    #data = data[length-length//fraction:]\n",
    "    \n",
    "    offset_sessions = np.zeros(data[session_key].nunique()+1, dtype=np.int32)\n",
    "    offset_sessions[1:] = data.groupby(session_key).size().cumsum() # arreglo con offset acumulativo de inicio de cada sesion\n",
    "    #offset_sessions = offset_sessions[length-length//fraction:]\n",
    "    \n",
    "    actual_session = 0 + offset\n",
    "    \n",
    "    batch_feats = None\n",
    "    batch_labels = None\n",
    "    # GRU_LAYER.reset_states() si usamos session parallel\n",
    "\n",
    "    while True:\n",
    "      datum = data[offset_sessions[actual_session]:offset_sessions[actual_session+1]][item_key]  # aqui toda la info de la sesion\n",
    "      datum = datum.values.reshape(-1,1)           \n",
    "      for i in range(offset_sessions[actual_session+1]-offset_sessions[actual_session]-1):\n",
    "        if not aug:\n",
    "          if (i != offset_sessions[actual_session+1]-offset_sessions[actual_session]-2):\n",
    "            continue\n",
    "        feats = datum[0:i+1]\n",
    "   \n",
    "        if feats.shape[0] > session_max_len:\n",
    "            feats = feats[:session_max_len] # aca cambiar a mas nuevos\n",
    "        else:\n",
    "            feats = np.append(np.zeros((session_max_len-feats.shape[0],1), dtype=np.int8), feats) # left pad with zeros\n",
    "\n",
    "        feats = feats.reshape(1,-1) # (1, 19)\n",
    "\n",
    "        label = datum[i+1]\n",
    "        label = np.expand_dims(label, axis=0)  # Termina siendo (1, dimn_previa)\n",
    "\n",
    "\n",
    "        if not isinstance(batch_feats, type(feats)):\n",
    "            batch_feats = feats\n",
    "        else:\n",
    "            batch_feats = np.append(batch_feats, feats, axis=0)\n",
    "\n",
    "        if not isinstance(batch_labels, type(label)):\n",
    "            batch_labels = label\n",
    "        else:\n",
    "            batch_labels = np.append(batch_labels, label, axis=0)\n",
    "\n",
    "        #print(batch_feats)\n",
    "        #print(batch_labels)\n",
    "        \n",
    "        if batch_labels.shape[0] == batch_size:\n",
    "          if not embedding:\n",
    "            # batch_labels.shape = (batch_size, 1)\n",
    "            #new_labels = np.zeros((batch_size, n_items))\n",
    "            #new_labels[0][:] = to_categorical(itemidmap[label[0][0]], num_classes=n_items)\n",
    "            batch_labels = to_categorical(itemidmap[batch_labels.flatten()], num_classes=n_items)\n",
    "          #print(\"Yielding batch with shape {} train, {} target\".format(batch_feats.shape, batch_labels.shape))\n",
    "            pass\n",
    "          \n",
    "          yield batch_feats, batch_labels\n",
    "          # resume batch generation\n",
    "          batch_feats = None\n",
    "          batch_labels = None\n",
    "\n",
    "    # TODO: Dropout random como en el paper\n",
    "\n",
    "      actual_session = (actual_session + 1) % len(offset_sessions)\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "eQslL5CpRjdQ",
    "outputId": "bd8d7ef2-638d-42a1-eaf5-6e96636a7be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items unicos training: 11619\n",
      "Items unicos dev: 10103\n",
      "Items unicos testing: 10365\n",
      "Sesiones training: 19850\n",
      "Sesiones validation: 5747\n",
      "Sesiones testing: 5270\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512 #como en el paper\n",
    "session_max_len = 100\n",
    "embeddingp=False\n",
    "\n",
    "n_items = len(train_data['ItemId'].unique())+1\n",
    "print(\"Items unicos training:\", n_items)\n",
    "\n",
    "dev_n_items = len(dev_data['ItemId'].unique())+1\n",
    "print(\"Items unicos dev:\", dev_n_items)\n",
    "\n",
    "test_n_items = len(test_data['ItemId'].unique())+1\n",
    "print(\"Items unicos testing:\", test_n_items)\n",
    "\n",
    "train_samples_qty = len(train_data['SessionId'].unique()) # cantidad sesiones no augmentadas de train\n",
    "print(\"Sesiones training:\", train_samples_qty)\n",
    "\n",
    "dev_samples_qty = len(dev_data['SessionId'].unique()) # cantidad sesiones no augmentadas de dev\n",
    "print(\"Sesiones validation:\",dev_samples_qty)\n",
    "\n",
    "test_samples_qty = len(test_data['SessionId'].unique()) # cantidad sesiones no augmentadas de test\n",
    "print(\"Sesiones testing:\", test_samples_qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1i_adI_ASgDi"
   },
   "outputs": [],
   "source": [
    "train_fraction = 1#256 # 1/fraction es la cantidad de sesiones mas recientes a considerar\n",
    "dev_fraction = 1#2\n",
    "\n",
    "train_offset_step=39\n",
    "dev_offset_step=12\n",
    "test_offset_step=11\n",
    "\n",
    "\n",
    "aux = [0]\n",
    "aux.extend(list(train_data['ItemId'].unique()))\n",
    "itemids = np.array(aux)\n",
    "itemidmap = pd.Series(data=np.arange(n_items), index=itemids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5z1JvtX8qV0"
   },
   "outputs": [],
   "source": [
    "test_generator = next(batch_generator(test_data, \n",
    "                                batch_size=batch_size, \n",
    "                                fraction=train_fraction, \n",
    "                                offset=test_offset_step,\n",
    "                               embedding=False,\n",
    "                                n_items=n_items,\n",
    "                               itemids=itemids,\n",
    "                               itemidmap=itemidmap,\n",
    "                                aug = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "O5_sa72xSF50",
    "outputId": "b1a69ec9-6e86-44ba-d19e-69fe1f59cf57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 50)           580950    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 50)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (None, 1000)              3156000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11619)             11630619  \n",
      "=================================================================\n",
      "Total params: 15,367,569\n",
      "Trainable params: 15,367,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcerdam/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/ipykernel_launcher.py:54: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "\n",
    "# ToDo:\n",
    "# meterle self-attention (hay implementaciones en Keras)\n",
    "\n",
    "def custom_cosine_loss(itemidmap, n_items):\n",
    "    #emb = model.layers[1]\n",
    "    emb = itemidmap\n",
    "    nu_items = n_items\n",
    "    # y_pred ya viene con embedding, y_true solo como one-hot\n",
    "    def fn(y_true, y_pred):\n",
    "        #print(y_true.shape, y_pred.shape)\n",
    "        y_pred_emb = to_categorical(emb[y_pred], num_classes=nu_items)\n",
    "        #print(y_true_emb)\n",
    "        #y_pred_emb = emb.call(y_pred)\n",
    "\n",
    "    #y_true_emb = np.array([y_true], dtype='int32')\n",
    "    #y_true_emb = tf.convert_to_tensor(y_true_emb)\n",
    "    #y_true_emb = model.layers[0].call(y_true)\n",
    "    #y_true_emb = K.get_value(y_true_emb)[0][0] # 50,\n",
    "\n",
    "        return 1 - cosine_proximity(y_true, y_pred_emb)\n",
    "        #return cosine_proximity(y_true_emb, y_pred_emb)\n",
    "    return fn\n",
    "    \n",
    "emb_size = 50\n",
    "size = emb_size\n",
    "#size = emb_size if embeddingp else n_items\n",
    "session_max_len=100\n",
    "\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "emb = Embedding(n_items, emb_size, embeddings_initializer='uniform', input_length=19)\n",
    "model.add(emb)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(CuDNNGRU(1000)) \n",
    "model.add(Dropout(0.25))\n",
    "if embeddingp:\n",
    "    model.add(Dense(emb_size, activation='softmax'))\n",
    "    custom_loss = custom_cosine_loss(emb)  ## DUDA: Esta usando los pesos actuales?\n",
    "    model.compile(loss=custom_loss, optimizer='adam')\n",
    "else:\n",
    "    model.add(Dense(n_items, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\"\"\"\n",
    "\n",
    "inputs = Input(shape=(session_max_len,))\n",
    "emb = Embedding(n_items, emb_size, embeddings_initializer='uniform', input_length=session_max_len)(inputs)\n",
    "drop1 = Dropout(0.25)(emb)\n",
    "gru = CuDNNGRU(1000)(drop1)\n",
    "drop2 = Dropout(0.25)(gru)\n",
    "predictions = Dense(n_items, activation='softmax')(drop2)\n",
    "model = Model(input=inputs, output=[predictions])\n",
    "custom_loss = custom_cosine_loss(itemidmap, n_items)\n",
    "# lr original es 0.0001\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# Try Nadam, too\n",
    "model.compile(loss=categorical_crossentropy, optimizer=opt)\n",
    "model.summary()\n",
    "\n",
    "#filepath='./bast/model_checkpoint'\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "callbacks_list = []#[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "1XFQrjF7TZlU",
    "outputId": "9c209770-d67a-4bb3-a0ac-ed2a2a65fc0e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "39/39 [==============================] - 13s 339ms/step - loss: 8.6324 - val_loss: 8.7593\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 13s 339ms/step - loss: 7.9533 - val_loss: 8.5514\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 14s 347ms/step - loss: 7.8655 - val_loss: 8.5526\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 13s 338ms/step - loss: 7.8583 - val_loss: 8.3973\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 13s 337ms/step - loss: 7.7202 - val_loss: 8.2548\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 13s 333ms/step - loss: 7.4693 - val_loss: 8.1903\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 13s 333ms/step - loss: 7.2588 - val_loss: 8.0555\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 13s 334ms/step - loss: 7.0478 - val_loss: 7.9237\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 13s 333ms/step - loss: 6.9101 - val_loss: 7.7120\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 13s 334ms/step - loss: 6.7134 - val_loss: 7.6198\n"
     ]
    }
   ],
   "source": [
    "real_epoca = 1\n",
    "for epoch in range(10):\n",
    "    #filepath='./weights/model_{}'.format(real_epoca)\n",
    "    #model.load_weights('./weights/model_{}'.format(real_epoca-1))\n",
    "    #model.save_weights(filepath)\n",
    "    train_generator = batch_generator(train_data, \n",
    "                                      batch_size=batch_size, \n",
    "                                  session_max_len=session_max_len,\n",
    "                                      fraction=train_fraction, \n",
    "                                      offset=train_offset_step*epoch,\n",
    "                                     embedding=embeddingp,\n",
    "                                      n_items=n_items,\n",
    "                                     itemids=itemids,\n",
    "                                     itemidmap=itemidmap)\n",
    "    \n",
    "    dev_generator = batch_generator(dev_data, \n",
    "                                    batch_size=batch_size,\n",
    "                                  session_max_len=session_max_len,\n",
    "                                    fraction=dev_fraction, \n",
    "                                    offset=dev_offset_step*epoch,\n",
    "                                    embedding=embeddingp,\n",
    "                                    n_items=n_items,\n",
    "                                    itemids=itemids,\n",
    "                                     itemidmap=itemidmap)\n",
    "    \n",
    "    history = model.fit_generator(train_generator,\n",
    "                                steps_per_epoch=train_offset_step,\n",
    "                                epochs=1,\n",
    "                                validation_data=dev_generator,\n",
    "                                validation_steps=dev_offset_step,\n",
    "                                callbacks=callbacks_list)\n",
    "    \n",
    "    #model.save_weights(filepath)\n",
    "    \"\"\"\n",
    "    weights = model.layers[1].get_weights()[0]\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=20, algorithm='ball_tree').fit(weights)\n",
    "    distances, indices = nbrs.kneighbors(weights) # Vienen ya ordenados! # Shape (37484, 20)\n",
    "    # Paso 3: Dado un vector embedding arbitrario, obtener el item más cercano a éste. Aplicarla sobre los 20 anteriores.\n",
    "    from sklearn.metrics import recall_score\n",
    "\n",
    "    test_generator = batch_generator(test_data, \n",
    "                                      batch_size=batch_size, \n",
    "                                      fraction=train_fraction, \n",
    "                                      offset=test_offset_step,\n",
    "                                     embedding=embeddingp,\n",
    "                                      n_items=n_items,\n",
    "                                     itemids=itemids,\n",
    "                                     itemidmap=itemidmap)\n",
    "\n",
    "\n",
    "    n = 0\n",
    "    suma = 0\n",
    "    while True:\n",
    "          try:\n",
    "            test_batch = next(test_generator)\n",
    "            pred = model.predict(test_batch[0]) # batch_size, n_items => 512, 37484\n",
    "            label = test_batch[1]               \n",
    "\n",
    "            #print(pred.shape)\n",
    "            #print(label.shape) \n",
    "\n",
    "            for row_idx in range(test_batch[0].shape[0]):\n",
    "              pred_row = pred[row_idx] # 37484, #.reshape(1, -1) # 50,\n",
    "              label_row = label[row_idx]        #.reshape(1, -1) # 50,\n",
    "\n",
    "              #print(pred_row.shape)\n",
    "              #print(label_row.shape)\n",
    "\n",
    "              idx1 = pred_row.argsort()[-20:][::-1]\n",
    "              idx2 = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "              n += 1\n",
    "              #print(idx1)\n",
    "              #print(idx2)\n",
    "              if idx2[0] in idx1:\n",
    "                suma += 1\n",
    "\n",
    "          except:\n",
    "            break\n",
    "    print(\"Recall epoch {}: {}\".format(epoch, suma/n))\n",
    "    \"\"\"\n",
    "    real_epoca += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en el init\n",
      "lei el csv\n",
      "antes sort values\n",
      "despues sort values\n",
      "antes de sampling\n",
      "despues de sampling\n",
      "antes itemn indices\n",
      "despues itemn indices\n",
      "antes offsets\n",
      "despues offsets\n",
      "antes order session\n",
      "despues order session\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (100,) but got array with shape (11618,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-baaccf045463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#print(input_oh)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtarget_oh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1882\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1884\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1885\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1481\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1484\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (100,) but got array with shape (11618,)"
     ]
    }
   ],
   "source": [
    "dataset = SessionDataset(train_data)\n",
    "loader = SessionDataLoader(dataset, batch_size=batch_size)\n",
    "for epoch in range(1):\n",
    "    for input, target, mask in loader:\n",
    "        input_oh = to_categorical(input, num_classes=loader.n_items) \n",
    "        #print(input_oh)\n",
    "        target_oh = to_categorical(target, num_classes=loader.n_items)   \n",
    "        tr_loss = model.train_on_batch(input_oh, target_oh)\n",
    "        print(tr_loss)\n",
    "        #print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "12800\n",
      "25600\n",
      "38400\n",
      "51200\n",
      "64000\n",
      "76800\n",
      "89600\n",
      "102400\n",
      "115200\n",
      "128000\n",
      "140800\n",
      "153600\n",
      "166400\n",
      "Recall@10 epoch 9: 0.08086756175149701\n"
     ]
    }
   ],
   "source": [
    "weights = model.layers[1].get_weights()[0]\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# RECALL @ 10\n",
    "recall_k = 10\n",
    "\n",
    "#nbrs = NearestNeighbors(n_neighbors=recall_k, algorithm='ball_tree').fit(weights)\n",
    "#distances, indices = nbrs.kneighbors(weights) # Vienen ya ordenados! # Shape (37484, 20)\n",
    "# Paso 3: Dado un vector embedding arbitrario, obtener el item más cercano a éste. Aplicarla sobre los 20 anteriores.\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "test_generator = batch_generator(test_data, \n",
    "                                  batch_size=batch_size,\n",
    "                                  session_max_len=session_max_len,\n",
    "                                  fraction=train_fraction, \n",
    "                                  offset=0,\n",
    "                                 embedding=embeddingp,\n",
    "                                  n_items=n_items,\n",
    "                                 itemids=itemids,\n",
    "                                 itemidmap=itemidmap)\n",
    "\n",
    "\n",
    "n = 0\n",
    "suma = 0\n",
    "suma_baseline = 0\n",
    "while True:\n",
    "    try:\n",
    "        test_batch = next(test_generator)\n",
    "        pred = model.predict(test_batch[0]) # batch_size, n_items => 512, 37484\n",
    "        \n",
    "\n",
    "        label = test_batch[1]               \n",
    "\n",
    "        if n%100 == 0:\n",
    "            print(n)\n",
    "        #print(pred.shape)\n",
    "        #print(label.shape) \n",
    "\n",
    "        for row_idx in range(test_batch[0].shape[0]):\n",
    "          #print(test_batch[0][row_idx])\n",
    "          #baseline_pred = obj.recommend( str(test_batch[0][row_idx][-1]), 20 )\n",
    "          pred_row = pred[row_idx] # 37484, #.reshape(1, -1) # 50,\n",
    "          label_row = label[row_idx]        #.reshape(1, -1) # 50,\n",
    "\n",
    "          #print(pred_row.shape)\n",
    "          #print(label_row.shape)\n",
    "\n",
    "          idx1 = pred_row.argsort()[-recall_k:][::-1]\n",
    "          idx2 = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "          n += 1\n",
    "          #print(idx1)\n",
    "          #print(idx2)\n",
    "          if idx2[0] in idx1:\n",
    "            suma += 1\n",
    "\n",
    "          #if idx2[0] in baseline_pred:\n",
    "          #  suma_baseline += 1\n",
    "\n",
    "    except:\n",
    "        break\n",
    "print(\"Recall@{} epoch {}: {}\".format(recall_k, epoch, suma/n))\n",
    "\n",
    "#print(\"Recall@{} baseline: {}\".format(recall_k, suma_baseline/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall @20 epoch 99: 0.08705440681137724\n",
    "\n",
    "Con session_max_len = 100:\n",
    "Recall @20 epoch 9: 0.12075458458083832\n",
    "\n",
    "Con dwell_time NO FUNCIONA BIEN. Hacer ese supuesto en este dataset no tiene sentido.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IIC3633_M1_Colab_V2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
