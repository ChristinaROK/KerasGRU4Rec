{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhMT2zYloMpm"
   },
   "source": [
    "#**Proyecto - Sistemas Recomendadores - IIC3633**\n",
    "\n",
    "## Implementaci칩n en Keras de Session-Based RNNs for Recommendation con soft atenttion\n",
    "\n",
    "### V2: Implementaci칩n de embedding sobre one-hot vectors para entrenamiento m치s eficiente y modelo m치s chico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3kJRW-qQ17_Q",
    "outputId": "1e336f48-43aa-4929-cefd-63e02dee3449"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import psutil\n",
    "import humanize\n",
    "import pyreclab\n",
    "import GPUtil as GPU\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import cosine_proximity, categorical_crossentropy\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers.core import Permute, Reshape, RepeatVector\n",
    "from keras.layers import Input, Dense, Dropout, CuDNNGRU, Embedding, concatenate, Lambda, multiply, merge, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfW00EfwSNQ6"
   },
   "outputs": [],
   "source": [
    "# Cargamos dataframes preprocesados de MovieLens20MM\n",
    "PATH_TO_TRAIN = './data/train.csv' #all_\n",
    "PATH_TO_DEV = './data/dev.csv'\n",
    "PATH_TO_TEST = './data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
    "dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'ItemId':np.int64})\n",
    "test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>186</td>\n",
       "      <td>1267347706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>858</td>\n",
       "      <td>1236356241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>912</td>\n",
       "      <td>1283426281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>1221</td>\n",
       "      <td>1236356224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>1230</td>\n",
       "      <td>1236293194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SessionId  ItemId        Time\n",
       "0         18     186  1267347706\n",
       "1         18     858  1236356241\n",
       "2         18     912  1283426281\n",
       "3         18    1221  1236356224\n",
       "4         18    1230  1236293194"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    def __init__(self, data, sep='\\t', session_key='SessionId', item_key='ItemId', time_key='Time', n_samples=-1, itemmap=None, time_sort=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
    "            itemmap: mapping between item IDs and item indices\n",
    "            time_sort: whether to sort the sessions by time or not\n",
    "        \"\"\"\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "\n",
    "        #Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        #clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "        \n",
    "        \n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "\n",
    "        return offsets\n",
    "    \n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\" Order the session indices \"\"\"\n",
    "        if self.time_sort:\n",
    "            # starting time for each sessions, sorted by session IDs\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            # order the session indices by session starting times\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "\n",
    "        return session_idx_arr\n",
    "    \n",
    "    \n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\" \n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # unique item ids\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            itemmap = pd.DataFrame({self.item_key:item_ids,\n",
    "                                   'item_idx':item2idx[item_ids].values})\n",
    "        \n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "        \n",
    "    \n",
    "    @property    \n",
    "    def items(self):\n",
    "        return self.itemmap.ItemId.unique()\n",
    "        \n",
    "\n",
    "class SessionDataLoader:\n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "             dataset (SessionDataset): the session dataset to generate the batches from\n",
    "             batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        # initializations\n",
    "        df = self.dataset.df\n",
    "        \n",
    "        print(df.head(n=100))\n",
    "        \n",
    "        session_key='SessionId'\n",
    "        item_key='ItemId'\n",
    "        time_key='TimeStamp'\n",
    "        self.n_items = df[item_key].nunique()+1\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = [] # indicator for the sessions to be terminated\n",
    "        finished = False        \n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices(for embedding) for clicks where the first sessions start\n",
    "            idx_target = df.item_idx.values[start]\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                input = idx_input\n",
    "                target = idx_target\n",
    "                yield input, target, mask\n",
    "                \n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "eQslL5CpRjdQ",
    "outputId": "bd8d7ef2-638d-42a1-eaf5-6e96636a7be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items unicos training: 11619\n",
      "Items unicos dev: 10103\n",
      "Items unicos testing: 10365\n",
      "Sesiones training: 19850\n",
      "Sesiones validation: 5747\n",
      "Sesiones testing: 5270\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512 #como en el paper\n",
    "session_max_len = 100\n",
    "embeddingp=False\n",
    "\n",
    "n_items = len(train_data['ItemId'].unique())+1\n",
    "print(\"Items unicos training:\", n_items)\n",
    "\n",
    "dev_n_items = len(dev_data['ItemId'].unique())+1\n",
    "print(\"Items unicos dev:\", dev_n_items)\n",
    "\n",
    "test_n_items = len(test_data['ItemId'].unique())+1\n",
    "print(\"Items unicos testing:\", test_n_items)\n",
    "\n",
    "train_samples_qty = len(train_data['SessionId'].unique()) # cantidad sesiones no augmentadas de train\n",
    "print(\"Sesiones training:\", train_samples_qty)\n",
    "\n",
    "dev_samples_qty = len(dev_data['SessionId'].unique()) # cantidad sesiones no augmentadas de dev\n",
    "print(\"Sesiones validation:\",dev_samples_qty)\n",
    "\n",
    "test_samples_qty = len(test_data['SessionId'].unique()) # cantidad sesiones no augmentadas de test\n",
    "print(\"Sesiones testing:\", test_samples_qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1i_adI_ASgDi"
   },
   "outputs": [],
   "source": [
    "train_fraction = 1#256 # 1/fraction es la cantidad de sesiones mas recientes a considerar\n",
    "dev_fraction = 1#2\n",
    "\n",
    "train_offset_step=train_samples_qty//batch_size\n",
    "dev_offset_step=dev_samples_qty//batch_size\n",
    "test_offset_step=test_samples_qty//batch_size\n",
    "\n",
    "\n",
    "aux = [0]\n",
    "aux.extend(list(train_data['ItemId'].unique()))\n",
    "itemids = np.array(aux)\n",
    "itemidmap = pd.Series(data=np.arange(n_items), index=itemids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "O5_sa72xSF50",
    "outputId": "b1a69ec9-6e86-44ba-d19e-69fe1f59cf57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (512, 1, 11619)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       [(512, 100), (512, 100)]  3516300   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (512, 100)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (512, 11619)              1173519   \n",
      "=================================================================\n",
      "Total params: 4,689,819\n",
      "Trainable params: 4,689,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcerdam/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "\n",
    "# ToDo: self-attention\n",
    "\n",
    "def attention_3d_block(inputs, TIME_STEPS, SINGLE_ATTENTION_VECTOR=True):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul\n",
    "    \n",
    "emb_size = 50\n",
    "hidden_units = 100\n",
    "size = emb_size\n",
    "#size = emb_size if embeddingp else n_items\n",
    "\n",
    "inputs = Input(batch_shape=(batch_size, 1, n_items))\n",
    "#emb = Embedding(n_items, emb_size, embeddings_initializer='uniform', input_length=session_max_len)(inputs)\n",
    "#drop1 = Dropout(0.25)(emb)\n",
    "gru, gru_states = CuDNNGRU(hidden_units, stateful=True, return_state=True)(inputs)# drop1) #\n",
    "drop2 = Dropout(0.25)(gru)\n",
    "#attention_mul = attention_3d_block(drop2, session_max_len)\n",
    "#attention_mul = Flatten()(attention_mul)\n",
    "predictions = Dense(n_items, activation='softmax')(drop2)#(attention_mul)#\n",
    "model = Model(input=inputs, output=[predictions])\n",
    "#custom_loss = custom_cosine_loss(itemidmap, n_items)\n",
    "# lr original es 0.0001\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# Try Nadam, too\n",
    "model.compile(loss=categorical_crossentropy, optimizer=opt)\n",
    "model.summary()\n",
    "\n",
    "#filepath='./bast/model_checkpoint'\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "callbacks_list = []#[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states(model):\n",
    "    return [K.get_value(s) for s,_ in model.state_updates]\n",
    "\n",
    "def set_states(model, states):\n",
    "    for (d,_), s in zip(model.state_updates, states):\n",
    "        K.set_value(d, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(generator, model, recall_k=20):\n",
    "\n",
    "    n = 0\n",
    "    suma = 0\n",
    "    suma_baseline = 0\n",
    "\n",
    "    for feat, label, mask in generator:\n",
    "        input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
    "        input_oh = np.expand_dims(input_oh, axis=1)\n",
    "\n",
    "        target_oh = to_categorical(label, num_classes=loader.n_items)\n",
    "\n",
    "        pred = model.predict(input_oh, batch_size=batch_size)\n",
    "\n",
    "        if n%100 == 0:\n",
    "            try:\n",
    "                print(\"{}:{}\".format(n, suma/n))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for row_idx in range(feat.shape[0]):\n",
    "            #baseline_pred = obj.recommend( str(test_batch[0][row_idx][-1]), 20 )\n",
    "            pred_row = pred[row_idx] # 37484, #.reshape(1, -1) # 50,\n",
    "            label_row = target_oh[row_idx]        #.reshape(1, -1) # 50,\n",
    "\n",
    "            idx1 = pred_row.argsort()[-recall_k:][::-1]\n",
    "            idx2 = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "            n += 1\n",
    "            #print(idx1)\n",
    "            #print(idx2)\n",
    "            if idx2[0] in idx1:\n",
    "                suma += 1\n",
    "\n",
    "            #if idx2[0] in baseline_pred:\n",
    "            #  suma_baseline += 1\n",
    "\n",
    "    print(\"Recall@{} epoch {}: {}\".format(recall_k, epoch, suma/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n",
      "Epoch 0 last loss: 9.117029190063477\n",
      "12800:0.001171875\n",
      "25600:0.0005859375\n",
      "38400:0.000390625\n",
      "51200:0.00029296875\n",
      "64000:0.000234375\n",
      "76800:0.0001953125\n",
      "89600:0.0007366071428571429\n",
      "102400:0.00166015625\n",
      "115200:0.002560763888888889\n",
      "128000:0.0031328125\n",
      "140800:0.0029119318181818183\n",
      "153600:0.0026692708333333334\n",
      "166400:0.003894230769230769\n",
      "179200:0.005\n",
      "192000:0.00590625\n",
      "Recall@20 epoch 0: 0.006038326243455497\n",
      "Starting epoch 1...\n",
      "Epoch 1 last loss: 9.143878936767578\n",
      "12800:0.138125\n",
      "25600:0.113828125\n",
      "38400:0.09427083333333333\n",
      "51200:0.08814453125\n",
      "64000:0.090515625\n",
      "76800:0.08895833333333333\n",
      "89600:0.08435267857142857\n",
      "102400:0.083828125\n",
      "115200:0.08611111111111111\n",
      "128000:0.0853671875\n",
      "140800:0.08402698863636364\n",
      "153600:0.08501302083333333\n",
      "166400:0.0841826923076923\n",
      "179200:0.08396763392857143\n",
      "192000:0.08227083333333333\n",
      "Recall@20 epoch 1: 0.08143304155759162\n",
      "Starting epoch 2...\n",
      "Epoch 2 last loss: 8.178423881530762\n",
      "12800:0.408125\n",
      "25600:0.361484375\n",
      "38400:0.3076041666666667\n",
      "51200:0.30845703125\n",
      "64000:0.328375\n",
      "76800:0.34096354166666665\n",
      "89600:0.3469866071428571\n",
      "102400:0.35123046875\n",
      "115200:0.35758680555555555\n",
      "128000:0.3557265625\n",
      "140800:0.35433238636363634\n",
      "153600:0.35003255208333334\n",
      "166400:0.34326923076923077\n",
      "179200:0.3359933035714286\n",
      "192000:0.32390625\n",
      "Recall@20 epoch 2: 0.31985745255235604\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SessionDataset(train_data)\n",
    "\n",
    "#print(dataset.df.head())\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(\"Starting epoch {}...\".format(epoch))\n",
    "    loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
    "    for feat, target, mask in loader:\n",
    "                \n",
    "        input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
    "        input_oh = np.expand_dims(input_oh, axis=1)\n",
    "        \n",
    "        target_oh = to_categorical(target, num_classes=loader.n_items)\n",
    "        \n",
    "        tr_loss = model.train_on_batch(input_oh, target_oh)\n",
    "        \n",
    "        real_mask = np.ones((batch_size, 1))\n",
    "        for elt in mask:\n",
    "            real_mask[elt, :] = 0\n",
    "        \n",
    "        hidden_states = get_states(model)[0]#512,100    #get_states(model)[0]\n",
    "               \n",
    "        hidden_states = np.multiply(real_mask, hidden_states)\n",
    "        hidden_states = np.array(hidden_states, dtype=np.float32)\n",
    "        #hidden_states = np.expand_dims(hidden_states, axis=0)\n",
    "        \n",
    "        #set_states(model, hidden_states)\\):\n",
    "        #print(hidden_states.shape)\n",
    "        #K.set_value(model.layers[1].states, hidden_states)\n",
    "        model.layers[1].reset_states(hidden_states)\n",
    "\n",
    "    \n",
    "    print(\"Epoch {} last loss: {}\".format(epoch, tr_loss))\n",
    "    \n",
    "    test_dataset = SessionDataset(test_data)\n",
    "    test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    get_recall(test_generator, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SessionId  ItemId        Time  item_idx\n",
      "0          49    2302  1367549005         0\n",
      "1        1268    2302  1386899188         0\n",
      "2        1597    2302  1377895328         0\n",
      "3        2161    2302  1390601637         0\n",
      "4        3990    2302  1392030145         0\n",
      "5        4967    2302  1371530281         0\n",
      "6        4983    2302  1369601471         0\n",
      "7        5039    2302  1387623912         0\n",
      "8        6092    2302  1366831892         0\n",
      "9        6615    2302  1378716484         0\n",
      "10       7098    2302  1367237291         0\n",
      "11       7209    2302  1369090925         0\n",
      "12       7922    2302  1371954565         0\n",
      "13       8753    2302  1388952359         0\n",
      "14       9353    2302  1381790220         0\n",
      "15      10903    2302  1370880281         0\n",
      "16      16899    2302  1377453013         0\n",
      "17      17281    2302  1395356543         0\n",
      "18      21481    2302  1385223173         0\n",
      "19      21636    2302  1366332013         0\n",
      "20      27171    2302  1386896988         0\n",
      "21      28368    2302  1387651210         0\n",
      "22      32007    2302  1395438985         0\n",
      "23      32317    2302  1383949560         0\n",
      "24      32400    2302  1367702470         0\n",
      "25      33589    2302  1366688888         0\n",
      "26      33787    2302  1374986606         0\n",
      "27      35271    2302  1388790688         0\n",
      "28      36812    2302  1395490020         0\n",
      "29      36825    2302  1369627199         0\n",
      "..        ...     ...         ...       ...\n",
      "70      88185    2302  1375289431         0\n",
      "71      88421    2302  1369583422         0\n",
      "72      89188    2302  1391463819         0\n",
      "73      90089    2302  1386745505         0\n",
      "74      91093    2302  1372059645         0\n",
      "75      91820    2302  1394169785         0\n",
      "76      91982    2302  1378248265         0\n",
      "77      96715    2302  1374789362         0\n",
      "78      97382    2302  1391957825         0\n",
      "79      97762    2302  1394515916         0\n",
      "80      99704    2302  1377093998         0\n",
      "81     100667    2302  1369859301         0\n",
      "82     103205    2302  1368152714         0\n",
      "83     103416    2302  1380318733         0\n",
      "84     103796    2302  1390363745         0\n",
      "85     104218    2302  1378400952         0\n",
      "86     105116    2302  1385165658         0\n",
      "87     106240    2302  1383750227         0\n",
      "88     107198    2302  1368825093         0\n",
      "89     108357    2302  1382225837         0\n",
      "90     108877    2302  1367121623         0\n",
      "91     110357    2302  1374290577         0\n",
      "92     111457    2302  1365373451         0\n",
      "93     112357    2302  1378970728         0\n",
      "94     117237    2302  1367141821         0\n",
      "95     121082    2302  1388290654         0\n",
      "96     122228    2302  1390241841         0\n",
      "97     129246    2302  1374604810         0\n",
      "98     129755    2302  1377999349         0\n",
      "99     131893    2302  1374445677         0\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3c9700fd218f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdev_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSessionDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mget_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-42c9653e89ce>\u001b[0m in \u001b[0;36mget_recall\u001b[0;34m(generator, model, recall_k)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mlabel_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_oh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;31m#.reshape(1, -1) # 50,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0midx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrecall_k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0midx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dev_dataset = SessionDataset(dev_data)\n",
    "dev_generator = SessionDataLoader(dev_dataset, batch_size=batch_size)\n",
    "    \n",
    "get_recall(dev_generator, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados Movie Lens con Batcher paralelo\n",
    "Recall@20 epoch 3: 0.252019592604712\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All train set\n",
    "Recall@10 epoch 29: 0.08087340943113773\n",
    "Recall@20: 0.10473194236526946\n",
    "\n",
    "vs Hidasi\n",
    "\n",
    "Recall @ 20 0.2177499329156604\n",
    "MRR@20: 0.06513681594077811\n",
    "\n",
    "Pruebas atencion\n",
    "Baseline\n",
    "Recall@20 epoch 49: 0.09473825785928144\n",
    "MultAttn\n",
    "\n",
    "\n",
    "\n",
    "# Train Set\n",
    "Recall@10 epoch ..100?: 0.09546921781437126\n",
    "\n",
    "Recall@10 epoch 14: 0.06404879908501715\n",
    "\n",
    "Recall @20 epoch 99: 0.08705440681137724\n",
    "\n",
    "Con session_max_len = 100:\n",
    "\n",
    "Recall @20 epoch 9: 0.12195335890718563\n",
    "\n",
    "Con dwell_time NO FUNCIONA BIEN. Hacer ese supuesto en este dataset no tiene sentido.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IIC3633_M1_Colab_V2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
