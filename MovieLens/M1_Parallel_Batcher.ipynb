{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhMT2zYloMpm"
   },
   "source": [
    "#**Proyecto - Sistemas Recomendadores - IIC3633**\n",
    "\n",
    "## Implementación en Keras de Session-Based RNNs for Recommendation con soft atenttion\n",
    "\n",
    "### V2: Implementación de embedding sobre one-hot vectors para entrenamiento más eficiente y modelo más chico\n",
    "\n",
    "\n",
    "Preliminar: Configuración entorno GPUs, Google Drive, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3kJRW-qQ17_Q",
    "outputId": "1e336f48-43aa-4929-cefd-63e02dee3449"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import psutil\n",
    "import humanize\n",
    "import pyreclab\n",
    "import GPUtil as GPU\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import cosine_proximity, categorical_crossentropy\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input, Dense, Dropout, CuDNNGRU, Embedding, concatenate, Lambda, multiply\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfW00EfwSNQ6"
   },
   "outputs": [],
   "source": [
    "# Cargamos dataframes preprocesados de RSC15\n",
    "PATH_TO_TRAIN = './data/train.csv'\n",
    "PATH_TO_DEV = './data/dev.csv'\n",
    "PATH_TO_TEST = './data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
    "dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'ItemId':np.int64})\n",
    "test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    def __init__(self, data, sep='\\t', session_key='SessionId', item_key='ItemId', time_key='Time', n_samples=-1, itemmap=None, time_sort=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: path of the csv file\n",
    "            sep: separator for the csv\n",
    "            session_key, item_key, time_key: name of the fields corresponding to the sessions, items, time\n",
    "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
    "            itemmap: mapping between item IDs and item indices\n",
    "            time_sort: whether to sort the sessions by time or not\n",
    "        \"\"\"\n",
    "        #self.df = pd.read_csv(path, sep=sep, names=[session_key, item_key, time_key])\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        self.df.sort_values([session_key, time_key], inplace=True) \n",
    "        # sampling\n",
    "        #if n_samples > 0: self.df = self.df[:n_samples] \n",
    "        # Add item indices\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "        \"\"\"\n",
    "        Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "        \"\"\"\n",
    "\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "        \n",
    "        \n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "\n",
    "        return offsets\n",
    "    \n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\" Order the session indices \"\"\"\n",
    "        if self.time_sort:\n",
    "            # starting time for each sessions, sorted by session IDs\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            # order the session indices by session starting times\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "\n",
    "        return session_idx_arr\n",
    "    \n",
    "    \n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\" \n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # unique item ids\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            itemmap = pd.DataFrame({self.item_key:item_ids,\n",
    "                                   'item_idx':item2idx[item_ids].values})\n",
    "        \n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "        \n",
    "    \n",
    "    @property    \n",
    "    def items(self):\n",
    "        return self.itemmap.ItemId.unique()\n",
    "        \n",
    "\n",
    "class SessionDataLoader:\n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "             dataset (SessionDataset): the session dataset to generate the batches from\n",
    "             batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        # initializations\n",
    "        df = self.dataset.df\n",
    "        session_key='SessionId'\n",
    "        item_key='ItemId'\n",
    "        time_key='TimeStamp'\n",
    "        self.n_items = df[item_key].nunique()+1\n",
    "        print(self.n_items)\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = [] # indicator for the sessions to be terminated\n",
    "        finished = False        \n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices(for embedding) for clicks where the first sessions start\n",
    "            idx_target = df.item_idx.values[start]\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                input = idx_input\n",
    "                target = idx_target\n",
    "                #print(input)\n",
    "                #print(target)\n",
    "                yield input, target, mask\n",
    "                \n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            for idx in mask:\n",
    "                #print(idx)\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFkivhjnvrKO"
   },
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size=128, session_max_len=19, fraction=1, offset=0, embedding=True, n_items=None, itemids=None, itemidmap=None, aug = True):\n",
    "    item_key = 'ItemId'\n",
    "    session_key = 'SessionId'\n",
    "    time_key = 'Time'\n",
    "    \n",
    "    data = pd.merge(data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\n",
    "    \n",
    "    #print(\"Cantidad de samples: {}\".format(len(data)//fraction))\n",
    "\n",
    "    data.sort_values([session_key, time_key], inplace=True) # ordenamos por sesion\n",
    "\n",
    "    #data.sort_values([time_key], inplace=True)\n",
    "    length = len(data['ItemId'])\n",
    "    #data = data[length-length//fraction:]\n",
    "    \n",
    "    offset_sessions = np.zeros(data[session_key].nunique()+1, dtype=np.int32)\n",
    "    offset_sessions[1:] = data.groupby(session_key).size().cumsum() # arreglo con offset acumulativo de inicio de cada sesion\n",
    "    #offset_sessions = offset_sessions[length-length//fraction:]\n",
    "    \n",
    "    actual_session = 0 + offset\n",
    "    \n",
    "    batch_feats = None\n",
    "    batch_labels = None\n",
    "    # GRU_LAYER.reset_states() si usamos session parallel\n",
    "\n",
    "    while True:\n",
    "      datum = data[offset_sessions[actual_session]:offset_sessions[actual_session+1]][item_key]  # aqui toda la info de la sesion\n",
    "      datum = datum.values.reshape(-1,1)           \n",
    "      for i in range(offset_sessions[actual_session+1]-offset_sessions[actual_session]-1):\n",
    "        if not aug:\n",
    "          if (i != offset_sessions[actual_session+1]-offset_sessions[actual_session]-2):\n",
    "            continue\n",
    "        feats = datum[0:i+1]\n",
    "   \n",
    "        if feats.shape[0] > session_max_len:\n",
    "            feats = feats[:session_max_len] # aca cambiar a mas nuevos\n",
    "        else:\n",
    "            feats = np.append(np.zeros((session_max_len-feats.shape[0],1), dtype=np.int8), feats) # left pad with zeros\n",
    "\n",
    "        feats = feats.reshape(1,-1) # (1, 19)\n",
    "        feats = np.expand_dims(feats, axis=2)\n",
    "\n",
    "        label = datum[i+1]\n",
    "        label = np.expand_dims(label, axis=0)  # Termina siendo (1, dimn_previa)\n",
    "\n",
    "\n",
    "        if not isinstance(batch_feats, type(feats)):\n",
    "            batch_feats = feats\n",
    "        else:\n",
    "            batch_feats = np.append(batch_feats, feats, axis=0)\n",
    "\n",
    "        if not isinstance(batch_labels, type(label)):\n",
    "            batch_labels = label\n",
    "        else:\n",
    "            batch_labels = np.append(batch_labels, label, axis=0)\n",
    "\n",
    "        #print(batch_feats)\n",
    "        #print(batch_labels)\n",
    "        \n",
    "        if batch_labels.shape[0] == batch_size:\n",
    "          if not embedding:\n",
    "            # batch_labels.shape = (batch_size, 1)\n",
    "            #new_labels = np.zeros((batch_size, n_items))\n",
    "            #new_labels[0][:] = to_categorical(itemidmap[label[0][0]], num_classes=n_items)\n",
    "            batch_labels = to_categorical(itemidmap[batch_labels.flatten()], num_classes=n_items)\n",
    "          #print(\"Yielding batch with shape {} train, {} target\".format(batch_feats.shape, batch_labels.shape))\n",
    "            pass\n",
    "          \n",
    "          yield batch_feats, batch_labels\n",
    "          # resume batch generation\n",
    "          batch_feats = None\n",
    "          batch_labels = None\n",
    "\n",
    "    # TODO: Dropout random como en el paper\n",
    "\n",
    "      actual_session = (actual_session + 1) % len(offset_sessions)\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "eQslL5CpRjdQ",
    "outputId": "bd8d7ef2-638d-42a1-eaf5-6e96636a7be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items unicos training: 11619\n",
      "Items unicos dev: 10103\n",
      "Items unicos testing: 10365\n",
      "Sesiones training: 19850\n",
      "Sesiones validation: 5747\n",
      "Sesiones testing: 5270\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512 #como en el paper\n",
    "session_max_len = 100\n",
    "embeddingp=False\n",
    "\n",
    "n_items = len(train_data['ItemId'].unique())+1\n",
    "print(\"Items unicos training:\", n_items)\n",
    "\n",
    "dev_n_items = len(dev_data['ItemId'].unique())+1\n",
    "print(\"Items unicos dev:\", dev_n_items)\n",
    "\n",
    "test_n_items = len(test_data['ItemId'].unique())+1\n",
    "print(\"Items unicos testing:\", test_n_items)\n",
    "\n",
    "train_samples_qty = len(train_data['SessionId'].unique()) # cantidad sesiones no augmentadas de train\n",
    "print(\"Sesiones training:\", train_samples_qty)\n",
    "\n",
    "dev_samples_qty = len(dev_data['SessionId'].unique()) # cantidad sesiones no augmentadas de dev\n",
    "print(\"Sesiones validation:\",dev_samples_qty)\n",
    "\n",
    "test_samples_qty = len(test_data['SessionId'].unique()) # cantidad sesiones no augmentadas de test\n",
    "print(\"Sesiones testing:\", test_samples_qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1i_adI_ASgDi"
   },
   "outputs": [],
   "source": [
    "train_fraction = 1#256 # 1/fraction es la cantidad de sesiones mas recientes a considerar\n",
    "dev_fraction = 1#2\n",
    "\n",
    "train_offset_step=39\n",
    "dev_offset_step=12\n",
    "test_offset_step=11\n",
    "\n",
    "\n",
    "aux = [0]\n",
    "aux.extend(list(train_data['ItemId'].unique()))\n",
    "itemids = np.array(aux)\n",
    "itemidmap = pd.Series(data=np.arange(n_items), index=itemids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "O5_sa72xSF50",
    "outputId": "b1a69ec9-6e86-44ba-d19e-69fe1f59cf57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1, 1)              0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)       (None, 100)               30900     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11619)             1173519   \n",
      "=================================================================\n",
      "Total params: 1,204,419\n",
      "Trainable params: 1,204,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcerdam/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/ipykernel_launcher.py:54: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "\n",
    "# ToDo:\n",
    "# meterle self-attention (hay implementaciones en Keras)\n",
    "\n",
    "def custom_cosine_loss(itemidmap, n_items):\n",
    "    #emb = model.layers[1]\n",
    "    emb = itemidmap\n",
    "    nu_items = n_items\n",
    "    # y_pred ya viene con embedding, y_true solo como one-hot\n",
    "    def fn(y_true, y_pred):\n",
    "        #print(y_true.shape, y_pred.shape)\n",
    "        y_pred_emb = to_categorical(emb[y_pred], num_classes=nu_items)\n",
    "        #print(y_true_emb)\n",
    "        #y_pred_emb = emb.call(y_pred)\n",
    "\n",
    "    #y_true_emb = np.array([y_true], dtype='int32')\n",
    "    #y_true_emb = tf.convert_to_tensor(y_true_emb)\n",
    "    #y_true_emb = model.layers[0].call(y_true)\n",
    "    #y_true_emb = K.get_value(y_true_emb)[0][0] # 50,\n",
    "\n",
    "        return 1 - cosine_proximity(y_true, y_pred_emb)\n",
    "        #return cosine_proximity(y_true_emb, y_pred_emb)\n",
    "    return fn\n",
    "    \n",
    "emb_size = 50\n",
    "size = emb_size\n",
    "#size = emb_size if embeddingp else n_items\n",
    "session_max_len=1#100\n",
    "\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "emb = Embedding(n_items, emb_size, embeddings_initializer='uniform', input_length=19)\n",
    "model.add(emb)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(CuDNNGRU(1000)) \n",
    "model.add(Dropout(0.25))\n",
    "if embeddingp:\n",
    "    model.add(Dense(emb_size, activation='softmax'))\n",
    "    custom_loss = custom_cosine_loss(emb)  ## DUDA: Esta usando los pesos actuales?\n",
    "    model.compile(loss=custom_loss, optimizer='adam')\n",
    "else:\n",
    "    model.add(Dense(n_items, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\"\"\"\n",
    "\n",
    "inputs = Input(shape=(session_max_len, 1))\n",
    "#emb = Embedding(n_items, emb_size, embeddings_initializer='uniform', input_length=session_max_len)(inputs)\n",
    "#drop1 = Dropout(0.25)(emb)\n",
    "gru = CuDNNGRU(100)(inputs)\n",
    "drop2 = Dropout(0.25)(gru)\n",
    "predictions = Dense(n_items, activation='softmax')(drop2)\n",
    "model = Model(input=inputs, output=[predictions])\n",
    "custom_loss = custom_cosine_loss(itemidmap, n_items)\n",
    "# lr original es 0.0001\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# Try Nadam, too\n",
    "model.compile(loss=categorical_crossentropy, optimizer=opt)\n",
    "model.summary()\n",
    "\n",
    "#filepath='./bast/model_checkpoint'\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "callbacks_list = []#[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "1XFQrjF7TZlU",
    "outputId": "9c209770-d67a-4bb3-a0ac-ed2a2a65fc0e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "real_epoca = 1\n",
    "for epoch in range(10):\n",
    "    train_generator = batch_generator(train_data, \n",
    "                                      batch_size=batch_size, \n",
    "                                  session_max_len=session_max_len,\n",
    "                                      fraction=train_fraction, \n",
    "                                      offset=train_offset_step*epoch,\n",
    "                                     embedding=embeddingp,\n",
    "                                      n_items=n_items,\n",
    "                                     itemids=itemids,\n",
    "                                     itemidmap=itemidmap)\n",
    "    \n",
    "    dev_generator = batch_generator(dev_data, \n",
    "                                    batch_size=batch_size,\n",
    "                                  session_max_len=session_max_len,\n",
    "                                    fraction=dev_fraction, \n",
    "                                    offset=dev_offset_step*epoch,\n",
    "                                    embedding=embeddingp,\n",
    "                                    n_items=n_items,\n",
    "                                    itemids=itemids,\n",
    "                                     itemidmap=itemidmap)\n",
    "    \n",
    "    history = model.fit_generator(train_generator,\n",
    "                                steps_per_epoch=train_offset_step,\n",
    "                                epochs=1,\n",
    "                                validation_data=dev_generator,\n",
    "                                validation_steps=dev_offset_step,\n",
    "                                callbacks=callbacks_list)\n",
    "    \n",
    "    real_epoca += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SessionDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a6cf533b594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSessionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSessionDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SessionDataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = SessionDataset(train_data)\n",
    "loader = SessionDataLoader(dataset, batch_size=32)\n",
    "for epoch in range(1):\n",
    "    for inp, target, mask in loader:\n",
    "        print(inp.shape)\n",
    "        #input_oh = to_categorical(inp, num_classes=loader.n_items) \n",
    "        #print(input_oh.shape)\n",
    "        target_oh = to_categorical(target, num_classes=loader.n_items)   \n",
    "        print(target.shape)\n",
    "        print(target_oh.shape)\n",
    "        tr_loss = model.train_on_batch(inp, target_oh)\n",
    "        print(tr_loss)\n",
    "        #print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "12800\n",
      "25600\n",
      "38400\n",
      "51200\n",
      "64000\n",
      "76800\n",
      "89600\n",
      "102400\n",
      "115200\n",
      "128000\n",
      "140800\n",
      "153600\n",
      "166400\n",
      "Recall@10 epoch 9: 0.08086756175149701\n"
     ]
    }
   ],
   "source": [
    "weights = model.layers[1].get_weights()[0]\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# RECALL @ 10\n",
    "recall_k = 10\n",
    "\n",
    "#nbrs = NearestNeighbors(n_neighbors=recall_k, algorithm='ball_tree').fit(weights)\n",
    "#distances, indices = nbrs.kneighbors(weights) # Vienen ya ordenados! # Shape (37484, 20)\n",
    "# Paso 3: Dado un vector embedding arbitrario, obtener el item más cercano a éste. Aplicarla sobre los 20 anteriores.\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "test_generator = batch_generator(test_data, \n",
    "                                  batch_size=batch_size,\n",
    "                                  session_max_len=session_max_len,\n",
    "                                  fraction=train_fraction, \n",
    "                                  offset=0,\n",
    "                                 embedding=embeddingp,\n",
    "                                  n_items=n_items,\n",
    "                                 itemids=itemids,\n",
    "                                 itemidmap=itemidmap)\n",
    "\n",
    "\n",
    "n = 0\n",
    "suma = 0\n",
    "suma_baseline = 0\n",
    "while True:\n",
    "    try:\n",
    "        test_batch = next(test_generator)\n",
    "        pred = model.predict(test_batch[0]) # batch_size, n_items => 512, 37484\n",
    "        \n",
    "\n",
    "        label = test_batch[1]               \n",
    "\n",
    "        if n%100 == 0:\n",
    "            print(n)\n",
    "        #print(pred.shape)\n",
    "        #print(label.shape) \n",
    "\n",
    "        for row_idx in range(test_batch[0].shape[0]):\n",
    "          #print(test_batch[0][row_idx])\n",
    "          #baseline_pred = obj.recommend( str(test_batch[0][row_idx][-1]), 20 )\n",
    "          pred_row = pred[row_idx] # 37484, #.reshape(1, -1) # 50,\n",
    "          label_row = label[row_idx]        #.reshape(1, -1) # 50,\n",
    "\n",
    "          #print(pred_row.shape)\n",
    "          #print(label_row.shape)\n",
    "\n",
    "          idx1 = pred_row.argsort()[-recall_k:][::-1]\n",
    "          idx2 = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "          n += 1\n",
    "          #print(idx1)\n",
    "          #print(idx2)\n",
    "          if idx2[0] in idx1:\n",
    "            suma += 1\n",
    "\n",
    "          #if idx2[0] in baseline_pred:\n",
    "          #  suma_baseline += 1\n",
    "\n",
    "    except:\n",
    "        break\n",
    "print(\"Recall@{} epoch {}: {}\".format(recall_k, epoch, suma/n))\n",
    "\n",
    "#print(\"Recall@{} baseline: {}\".format(recall_k, suma_baseline/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall @20 epoch 99: 0.08705440681137724\n",
    "\n",
    "Con session_max_len = 100:\n",
    "Recall @20 epoch 9: 0.12075458458083832\n",
    "\n",
    "Con dwell_time NO FUNCIONA BIEN. Hacer ese supuesto en este dataset no tiene sentido.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IIC3633_M1_Colab_V2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
