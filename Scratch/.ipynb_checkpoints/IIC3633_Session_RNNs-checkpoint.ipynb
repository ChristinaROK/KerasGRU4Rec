{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhMT2zYloMpm"
   },
   "source": [
    "#**Proyecto - Sistemas Recomendadores - IIC3633**\n",
    "\n",
    "## Implementación en Keras de Session-Based RNNs for Recommendation con soft atenttion\n",
    "\n",
    "\n",
    "Preliminar: Configuración entorno GPUs, Google Drive, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kJRW-qQ17_Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import psutil\n",
    "import humanize\n",
    "import GPUtil as GPU\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ik4GlGz8oZvj",
    "outputId": "03c24b94-8ee0-412c-aeb0-285b9d88cfe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: cannot remove '/usr/bin/nvidia-smi': Permission denied\n",
      "Gen RAM Free: 14.8 GB  I Proc size: 237.3 MB\n",
      "GPU RAM Free 7805MB | Used: 300MB | Util   4% | Total          8105MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('/device:GPU:0',\n",
       "  <google.protobuf.pyext._message.MessageDescriptor at 0x7feb140636b0>,\n",
       "  2,\n",
       "  1,\n",
       "  7)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuracion GPUs\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "\n",
    "GPUs = GPU.getGPUs()\n",
    "gpu = GPUs[0]\n",
    "\n",
    "def print_gpu_info():\n",
    "  process = psutil.Process(os.getpid())\n",
    "  print(\"Gen RAM Free: \" + humanize.naturalsize(\n",
    "          psutil.virtual_memory().available), \" I Proc size: \"  +\n",
    "          humanize.naturalsize(process.memory_info().rss))\n",
    "  print(\"GPU RAM Free {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total \\\n",
    "         {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, \n",
    "                           gpu.memoryTotal))\n",
    "  \n",
    "print_gpu_info()\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [(x.name, x.DESCRIPTOR, x.DEVICE_TYPE_FIELD_NUMBER, x.NAME_FIELD_NUMBER, x.PHYSICAL_DEVICE_DESC_FIELD_NUMBER) for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pJtwaqa3oeWK",
    "outputId": "cdb18501-f031-4f1f-948a-4c57576da99a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import SimpleRNN, Dense, Flatten, Dropout, TimeDistributed, LSTM\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfW00EfwSNQ6"
   },
   "outputs": [],
   "source": [
    "# Cargamos dataframes preprocesados de RSC15\n",
    "PATH_TO_TRAIN = '../processedData/rsc15_train_tr.txt'\n",
    "PATH_TO_TEST = '../processedData/rsc15_train_valid.txt'\n",
    "\n",
    "data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
    "valid = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId':np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvPqM-J5T3HW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nitem_key = 'ItemId'\\nsession_key = 'SessionId'\\ntime_key = 'Time'\\n\\nitemids = data[item_key].unique()\\nn_items = len(itemids)\\n\\nitemidmap = pd.Series(data=np.arange(n_items), index=itemids) # Mapeo desde los 37.5k a (0, 37.5k) id\\nmdata = pd.merge(data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\\n\\nmdata.sort_values([session_key, time_key], inplace=True) # ordenamos por sesion\\noffset_sessions = np.zeros(mdata[session_key].nunique()+1, dtype=np.int32)\\noffset_sessions[1:] = mdata.groupby(session_key).size().cumsum() # arreglo con offset acumulativo de inicio de cada sesion\\nactual_session = 0\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pruebas\n",
    "\"\"\"\n",
    "item_key = 'ItemId'\n",
    "session_key = 'SessionId'\n",
    "time_key = 'Time'\n",
    "\n",
    "itemids = data[item_key].unique()\n",
    "n_items = len(itemids)\n",
    "\n",
    "itemidmap = pd.Series(data=np.arange(n_items), index=itemids) # Mapeo desde los 37.5k a (0, 37.5k) id\n",
    "mdata = pd.merge(data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\n",
    "\n",
    "mdata.sort_values([session_key, time_key], inplace=True) # ordenamos por sesion\n",
    "offset_sessions = np.zeros(mdata[session_key].nunique()+1, dtype=np.int32)\n",
    "offset_sessions[1:] = mdata.groupby(session_key).size().cumsum() # arreglo con offset acumulativo de inicio de cada sesion\n",
    "actual_session = 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "yDwZbzzgXl1Y",
    "outputId": "768aa960-5bbf-44e2-9460-aa373d221529"
   },
   "outputs": [],
   "source": [
    "#mdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SpGRmKjhX-Kv",
    "outputId": "fb2d662f-328c-4e80-ce14-bbcf56d86027"
   },
   "outputs": [],
   "source": [
    "#list(mdata[mdata['ItemId']==214536500]['ItemIdx'])[0]\n",
    "#print(mdata[mdata['ItemId']==214652220]['ItemIdx'].unique()[0])\n",
    "#print(mdata[mdata['ItemId']==214652220]['ItemIdx'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ryuCgz-q4BFq",
    "outputId": "211117b1-16b4-4d93-b4c3-9c5f3ed2ee4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nactual_session = 0\\n\\nbatch_size = 128\\nbatch_feats = None\\nbatch_labels = None\\n\\n# Entrega tensores de shape (batch_size, n_items+1, 19)\\n\\nwhile True:\\n    datum = mdata[offset_sessions[actual_session]:offset_sessions[actual_session+1]][item_key]  # aqui toda la info de la sesion\\n    datum = datum.values.reshape(-1,1)\\n    \\n    for i in range(offset_sessions[actual_session+1]-offset_sessions[actual_session]-1):\\n        \\n        feats = datum[0:i+1]\\n        if feats.shape[0] > 19:\\n            feats = feats[:19]\\n        else:\\n            feats = np.append(np.zeros((19-feats.shape[0],1), dtype=np.int8), feats) # left pad with zeros\\n\\n        encoded_feats = None\\n        for elt in feats:\\n            if elt == 0:\\n                idx = n_items\\n            else:\\n                idx = mdata[mdata['ItemId']==elt]['ItemIdx'].unique()[0]\\n            encoded = to_categorical(idx, num_classes=n_items+1)\\n            encoded = encoded.reshape(1, -1)\\n            \\n            if not isinstance(encoded_feats, type(feats)):\\n                encoded_feats = encoded\\n            else:\\n                encoded_feats = np.append(encoded_feats, encoded, axis=0) # Termina siendo de (19, n_items)\\n                \\n        label_idx = mdata[mdata['ItemId']==datum[i+1][0]]['ItemIdx'].unique()[0]\\n        label = to_categorical(label_idx, num_classes=n_items+1)\\n        label = np.expand_dims(label, axis=0)  # Termina siendo (1, n_items)\\n\\n        if not isinstance(batch_feats, type(feats)):\\n            batch_feats = np.expand_dims(encoded_feats, axis=0)\\n        else:\\n            batch_feats = np.append(batch_feats, np.expand_dims(encoded_feats, axis=0), axis=0)\\n\\n        if not isinstance(batch_labels, type(label)):\\n            batch_labels = np.expand_dims(label, axis=0)\\n        else:\\n            batch_labels = np.append(batch_labels, np.expand_dims(label, axis=0), axis=0)\\n\\n        if batch_labels.shape[0] == batch_size:\\n            print(batch_feats.shape)\\n            print(batch_labels.shape)\\n            break\\n    \\n    if batch_labels.shape[0] == batch_size:\\n        break\\n        \\n    actual_session += 1\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pruebas\n",
    "\"\"\"\n",
    "actual_session = 0\n",
    "\n",
    "batch_size = 128\n",
    "batch_feats = None\n",
    "batch_labels = None\n",
    "\n",
    "# Entrega tensores de shape (batch_size, n_items+1, 19)\n",
    "\n",
    "while True:\n",
    "    datum = mdata[offset_sessions[actual_session]:offset_sessions[actual_session+1]][item_key]  # aqui toda la info de la sesion\n",
    "    datum = datum.values.reshape(-1,1)\n",
    "    \n",
    "    for i in range(offset_sessions[actual_session+1]-offset_sessions[actual_session]-1):\n",
    "        \n",
    "        feats = datum[0:i+1]\n",
    "        if feats.shape[0] > 19:\n",
    "            feats = feats[:19]\n",
    "        else:\n",
    "            feats = np.append(np.zeros((19-feats.shape[0],1), dtype=np.int8), feats) # left pad with zeros\n",
    "\n",
    "        encoded_feats = None\n",
    "        for elt in feats:\n",
    "            if elt == 0:\n",
    "                idx = n_items\n",
    "            else:\n",
    "                idx = mdata[mdata['ItemId']==elt]['ItemIdx'].unique()[0]\n",
    "            encoded = to_categorical(idx, num_classes=n_items+1)\n",
    "            encoded = encoded.reshape(1, -1)\n",
    "            \n",
    "            if not isinstance(encoded_feats, type(feats)):\n",
    "                encoded_feats = encoded\n",
    "            else:\n",
    "                encoded_feats = np.append(encoded_feats, encoded, axis=0) # Termina siendo de (19, n_items)\n",
    "                \n",
    "        label_idx = mdata[mdata['ItemId']==datum[i+1][0]]['ItemIdx'].unique()[0]\n",
    "        label = to_categorical(label_idx, num_classes=n_items+1)\n",
    "        label = np.expand_dims(label, axis=0)  # Termina siendo (1, n_items)\n",
    "\n",
    "        if not isinstance(batch_feats, type(feats)):\n",
    "            batch_feats = np.expand_dims(encoded_feats, axis=0)\n",
    "        else:\n",
    "            batch_feats = np.append(batch_feats, np.expand_dims(encoded_feats, axis=0), axis=0)\n",
    "\n",
    "        if not isinstance(batch_labels, type(label)):\n",
    "            batch_labels = np.expand_dims(label, axis=0)\n",
    "        else:\n",
    "            batch_labels = np.append(batch_labels, np.expand_dims(label, axis=0), axis=0)\n",
    "\n",
    "        if batch_labels.shape[0] == batch_size:\n",
    "            print(batch_feats.shape)\n",
    "            print(batch_labels.shape)\n",
    "            break\n",
    "    \n",
    "    if batch_labels.shape[0] == batch_size:\n",
    "        break\n",
    "        \n",
    "    actual_session += 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFkivhjnvrKO"
   },
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size=128, session_max_len=19):\n",
    "    item_key = 'ItemId'\n",
    "    session_key = 'SessionId'\n",
    "    time_key = 'Time'\n",
    "    \n",
    "    itemids = data[item_key].unique()\n",
    "    n_items = len(itemids)\n",
    "    \n",
    "    itemidmap = pd.Series(data=np.arange(n_items), index=itemids) # Mapeo desde los 37.5k a (0, 37.5k) id\n",
    "    data = pd.merge(data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\n",
    "\n",
    "    data.sort_values([session_key, time_key], inplace=True) # ordenamos por sesion\n",
    "    offset_sessions = np.zeros(data[session_key].nunique()+1, dtype=np.int32)\n",
    "    offset_sessions[1:] = data.groupby(session_key).size().cumsum() # arreglo con offset acumulativo de inicio de cada sesion\n",
    "    actual_session = 0\n",
    "    \n",
    "    batch_feats = None\n",
    "    batch_labels = None\n",
    "\n",
    "    while True:\n",
    "        datum = data[offset_sessions[actual_session]:offset_sessions[actual_session+1]][item_key]  # aqui toda la info de la sesion\n",
    "        datum = datum.values.reshape(-1,1)\n",
    "        \n",
    "        for i in range(offset_sessions[actual_session+1]-offset_sessions[actual_session]-1):\n",
    "            feats = datum[0:i+1]\n",
    "            if feats.shape[0] > session_max_len:\n",
    "                feats = feats[:session_max_len]\n",
    "            else:\n",
    "                feats = np.append(np.zeros((session_max_len-feats.shape[0],1), dtype=np.int8), feats) # left pad with zeros\n",
    "            \n",
    "            encoded_feats = None\n",
    "            for elt in feats:\n",
    "                if elt == 0:\n",
    "                    idx = n_items\n",
    "                else:\n",
    "                    try:\n",
    "                        idx = data[data['ItemId']==elt]['ItemIdx'].unique()[0]\n",
    "                    except:\n",
    "                        idx = data[data['ItemId']==elt[0]]['ItemIdx'].unique()[0]\n",
    "                encoded = to_categorical(idx, num_classes=n_items+1)\n",
    "                encoded = encoded.reshape(1, -1)\n",
    "\n",
    "                if not isinstance(encoded_feats, type(feats)):\n",
    "                    encoded_feats = encoded\n",
    "                else:\n",
    "                    encoded_feats = np.append(encoded_feats, encoded, axis=0) # Termina siendo de (19, n_items)\n",
    "\n",
    "            label_idx = data[data['ItemId']==datum[i+1][0]]['ItemIdx'].unique()[0]\n",
    "            label = to_categorical(label_idx, num_classes=n_items+1)\n",
    "            label = np.expand_dims(label, axis=0)  # Termina siendo (1, n_items)\n",
    "            \n",
    "            if not isinstance(batch_feats, type(feats)):\n",
    "                batch_feats = np.expand_dims(encoded_feats, axis=0)\n",
    "            else:\n",
    "                batch_feats = np.append(batch_feats, np.expand_dims(encoded_feats, axis=0), axis=0)\n",
    "\n",
    "            if not isinstance(batch_labels, type(label)):\n",
    "                batch_labels = label #np.expand_dims(label, axis=0)\n",
    "            else:\n",
    "                batch_labels = np.append(batch_labels, label, axis=0)#np.expand_dims(label, axis=0), axis=0)\n",
    "\n",
    "            if batch_labels.shape[0] == batch_size:\n",
    "                # return batch\n",
    "                #print(\"Yielding batch with shape {} train, {} target\".format(batch_feats.shape, batch_labels.shape))\n",
    "                yield batch_feats, batch_labels\n",
    "                \n",
    "                # resume batch generation\n",
    "                batch_feats = None\n",
    "                batch_labels = None\n",
    "            \n",
    "        # TODO: Dropout\n",
    "        \n",
    "        actual_session = (actual_session + 1) % len(offset_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eQslL5CpRjdQ",
    "outputId": "4ce1fec7-c4da-4a83-b12d-10a401bc2827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37484\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128 # 512\n",
    "session_max_len = 19\n",
    "n_items = len(data['ItemId'].unique())+1\n",
    "print(n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "AdM3tjmSbxjG",
    "outputId": "d3b74402-9430-4cff-fbc7-d177330bfd2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnngru_1 (CuDNNGRU)       (None, 10)                1124880   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 37484)             412324    \n",
      "=================================================================\n",
      "Total params: 1,537,204\n",
      "Trainable params: 1,537,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "# ToDo: reimplementar paper\n",
    "# meterle self-attention (hay implementaciones en Keras)\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, Dense, Dropout, CuDNNGRU, GRU, Embedding, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#model.add(Embedding(n_items, 50, input_length=19)) # input_length=19,\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(CuDNNGRU(10, input_shape=(19, n_items))) # Probar con 100 y 1000 una vez que tenga los embeddings. Sin ellos se cae por memoria \n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(n_items, activation='softmax'))\n",
    "#model.add(Dropout(0.2)) # Probar esto mas adelante\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "filepath=\"model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1i_adI_ASgDi"
   },
   "outputs": [],
   "source": [
    "train_generator = batch_generator(data, batch_size=batch_size)\n",
    "\n",
    "dev_generator = batch_generator(valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7FvXrjm3Sg1F",
    "outputId": "29620367-8439-4930-a8bc-b0a328238b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "499/500 [============================>.] - ETA: 36s - loss: 8.8895 "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected cu_dnngru_1_input to have shape (19, 37484) but got array with shape (19, 6360)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-02695463ee8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                               callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2242\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m                                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m                                 max_queue_size=max_queue_size)\n\u001b[0m\u001b[1;32m   2245\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2376\u001b[0m                                      \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m                                      str(generator_output))\n\u001b[0;32m-> 2378\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m   1917\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected cu_dnngru_1_input to have shape (19, 37484) but got array with shape (19, 6360)"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=10,\n",
    "                              validation_data=dev_generator,\n",
    "                              validation_steps=35,\n",
    "                              callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "11GFp9PLSwKd"
   },
   "outputs": [],
   "source": [
    "insfile = drive.CreateFile({'title': 'model.hdf5'})\n",
    "file.SetContentFile('./models.hdf5')\n",
    "file.Upload() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUg_4o1PbIRw"
   },
   "outputs": [],
   "source": [
    "3   10.5315\n",
    "4   10.5314\n",
    "8   10.5305\n",
    "13  10.5290\n",
    "21  10.5265\n",
    "62  10.4782\n",
    "74  10.4348\n",
    "101 10.3140\n",
    "114 10.2392\n",
    "165  9.9084 x\n",
    "174  9.8366\n",
    "287  9.3494\n",
    "499  8.8895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0kfDGI2FNSh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IIC3633 Session RNNs.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
