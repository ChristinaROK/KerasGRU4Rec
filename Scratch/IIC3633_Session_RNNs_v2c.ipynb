{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhMT2zYloMpm"
   },
   "source": [
    "#**Proyecto - Sistemas Recomendadores - IIC3633**\n",
    "\n",
    "## Implementación en Keras de Session-Based RNNs for Recommendation con soft atenttion\n",
    "\n",
    "### V2: Implementación de embedding sobre one-hot vectors para entrenamiento más eficiente y modelo más chico\n",
    "\n",
    "\n",
    "Preliminar: Configuración entorno GPUs, Google Drive, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "Z5dA3-L0nfLG",
    "outputId": "ace05f26-42f3-41a7-8911-f93c2709d85c"
   },
   "outputs": [],
   "source": [
    "# Manejo de Google Drive\n",
    "!pip install -U -q PyDrive\n",
    "\n",
    "from google.colab import drive, auth\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "6LGXgPsyoWsr",
    "outputId": "8a0b95ca-b0fc-44ba-8f55-e57a36e72c3e"
   },
   "outputs": [],
   "source": [
    "# Librerias varias\n",
    "\n",
    "!pip install gputil\n",
    "!pip install humanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kJRW-qQ17_Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import psutil\n",
    "import humanize\n",
    "import GPUtil as GPU\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import cosine_proximity, categorical_crossentropy\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input, Dense, Dropout, CuDNNGRU, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "ik4GlGz8oZvj",
    "outputId": "177b90ae-a085-45ce-edd8-8769eb672211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen RAM Free: 14.2 GB  I Proc size: 237.6 MB\n",
      "GPU RAM Free 7682MB | Used: 423MB | Util   5% | Total          8105MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('/device:GPU:0',\n",
       "  <google.protobuf.pyext._message.MessageDescriptor at 0x7fb92aee16b0>,\n",
       "  2,\n",
       "  1,\n",
       "  7)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuracion GPUs\n",
    "#!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "\n",
    "GPUs = GPU.getGPUs()\n",
    "gpu = GPUs[0]\n",
    "\n",
    "def print_gpu_info():\n",
    "  process = psutil.Process(os.getpid())\n",
    "  print(\"Gen RAM Free: \" + humanize.naturalsize(\n",
    "          psutil.virtual_memory().available), \" I Proc size: \"  +\n",
    "          humanize.naturalsize(process.memory_info().rss))\n",
    "  print(\"GPU RAM Free {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total \\\n",
    "         {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, \n",
    "                           gpu.memoryTotal))\n",
    "  \n",
    "print_gpu_info()\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [(x.name, x.DESCRIPTOR, x.DEVICE_TYPE_FIELD_NUMBER, x.NAME_FIELD_NUMBER, x.PHYSICAL_DEVICE_DESC_FIELD_NUMBER) for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfW00EfwSNQ6"
   },
   "outputs": [],
   "source": [
    "# Cargamos dataframes preprocesados de RSC15\n",
    "PATH_TO_TRAIN = '../processedData/rsc15_train_tr.txt'\n",
    "PATH_TO_DEV = '../processedData/rsc15_train_valid.txt'\n",
    "PATH_TO_TEST = '../processedData/rsc15_test.txt'\n",
    "\n",
    "train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
    "dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'ItemId':np.int64})\n",
    "test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFkivhjnvrKO"
   },
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size=128, session_max_len=19, fraction=1, offset=0, embedding=True, n_items=None, itemids=None, itemidmap=None):\n",
    "    item_key = 'ItemId'\n",
    "    session_key = 'SessionId'\n",
    "    time_key = 'Time'\n",
    "    \n",
    "    #if not embedding:\n",
    "    #    itemidmap=itemidmap\n",
    "    #else:\n",
    "    #    itemids = data[item_key].unique()\n",
    "    #    print(itemids.shape)\n",
    "    #    n_items = len(itemids)\n",
    "    #    # Mapeo desde los 37.5k a (0, 37.5k) id\n",
    "    #    itemidmap = pd.Series(data=np.arange(n_items), index=itemids) \n",
    "    \n",
    "    \n",
    "    data = pd.merge(data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\n",
    "    \n",
    "    #print(\"Cantidad de samples: {}\".format(len(data)//fraction))\n",
    "\n",
    "    data.sort_values([session_key, time_key], inplace=True) # ordenamos por sesion\n",
    "\n",
    "    #data.sort_values([time_key], inplace=True)\n",
    "    length = len(data['ItemId'])\n",
    "    #data = data[length-length//fraction:]\n",
    "    \n",
    "    offset_sessions = np.zeros(data[session_key].nunique()+1, dtype=np.int32)\n",
    "    offset_sessions[1:] = data.groupby(session_key).size().cumsum() # arreglo con offset acumulativo de inicio de cada sesion\n",
    "    #offset_sessions = offset_sessions[length-length//fraction:]\n",
    "    \n",
    "    actual_session = 0 + offset\n",
    "    \n",
    "    batch_feats = None\n",
    "    batch_labels = None\n",
    "    \n",
    "    # GRU_LAYER.reset_states() \n",
    "\n",
    "    while True:\n",
    "        datum = data[offset_sessions[actual_session]:offset_sessions[actual_session+1]][item_key]  # aqui toda la info de la sesion\n",
    "        datum = datum.values.reshape(-1,1)\n",
    "        \n",
    "        for i in range(offset_sessions[actual_session+1]-offset_sessions[actual_session]-1):\n",
    "            feats = datum[0:i+1]\n",
    "            if feats.shape[0] > session_max_len:\n",
    "                feats = feats[:session_max_len] # aca cambiar a mas nuevos\n",
    "            else:\n",
    "                feats = np.append(np.zeros((session_max_len-feats.shape[0],1), dtype=np.int8), feats) # left pad with zeros\n",
    "                      \n",
    "            feats = feats.reshape(1,-1) # (1, 19)\n",
    "            \n",
    "            #if False:#not embedding:\n",
    "            #    new_feats = np.zeros((1, session_max_len, n_items))\n",
    "            #    for item_idx in range(feats[0].shape[0]):\n",
    "            #        new_feats[0][item_idx][:] = to_categorical(itemidmap[feats[0][item_idx]], num_classes=n_items)\n",
    "            #    feats = new_feats\n",
    "                    \n",
    "            label = datum[i+1]\n",
    "            label = np.expand_dims(label, axis=0)  # Termina siendo (1, dimn_previa)\n",
    "            \n",
    "            if not embedding:\n",
    "                new_label = np.zeros((1, n_items))\n",
    "                new_label[0][:] = to_categorical(itemidmap[label[0][0]], num_classes=n_items)\n",
    "                label = new_label\n",
    "            \n",
    "            if not isinstance(batch_feats, type(feats)):\n",
    "                batch_feats = feats\n",
    "            else:\n",
    "                batch_feats = np.append(batch_feats, feats, axis=0)\n",
    "\n",
    "            if not isinstance(batch_labels, type(label)):\n",
    "                batch_labels = label #np.expand_dims(label, axis=0)\n",
    "            else:\n",
    "                batch_labels = np.append(batch_labels, label, axis=0)#np.expand_dims(label, axis=0), axis=0)\n",
    "\n",
    "            if batch_labels.shape[0] == batch_size:\n",
    "                #print(\"Yielding batch with shape {} train, {} target\".format(batch_feats.shape, batch_labels.shape))\n",
    "                yield batch_feats, batch_labels\n",
    "                \n",
    "                # resume batch generation\n",
    "                batch_feats = None\n",
    "                batch_labels = None\n",
    "            \n",
    "        # TODO: Dropout random como en el paper\n",
    "        \n",
    "        actual_session = (actual_session + 1) % len(offset_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "eQslL5CpRjdQ",
    "outputId": "6d2fc6bf-34f8-446b-91b2-34e605ba9974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items unicos training: 37484\n",
      "Items unicos dev: 6360\n",
      "Items unicos testing: 6752\n",
      "Sesiones training: 7953885\n",
      "Sesiones validation: 12372\n",
      "Sesiones testing: 15324\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512 # como en el paper\n",
    "session_max_len = 19\n",
    "embeddingp=True\n",
    "\n",
    "n_items = len(train_data['ItemId'].unique())+1\n",
    "print(\"Items unicos training:\", n_items)\n",
    "\n",
    "dev_n_items = len(dev_data['ItemId'].unique())+1\n",
    "print(\"Items unicos dev:\", dev_n_items)\n",
    "\n",
    "test_n_items = len(test_data['ItemId'].unique())+1\n",
    "print(\"Items unicos testing:\", test_n_items)\n",
    "\n",
    "train_samples_qty = len(train_data['SessionId'].unique()) # cantidad sesiones no augmentadas de train\n",
    "print(\"Sesiones training:\", train_samples_qty)\n",
    "\n",
    "dev_samples_qty = len(dev_data['SessionId'].unique()) # cantidad sesiones no augmentadas de dev\n",
    "print(\"Sesiones validation:\",dev_samples_qty)\n",
    "\n",
    "test_samples_qty = len(test_data['SessionId'].unique()) # cantidad sesiones no augmentadas de test\n",
    "print(\"Sesiones testing:\", test_samples_qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "AdM3tjmSbxjG",
    "outputId": "a4279c1b-f3e3-474c-a8bf-9f2c24386c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_2_target:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"loss_1/dense_2_loss/strided_slice_1:0\", shape=(50,), dtype=float32)\n",
      "Tensor(\"dense_2/Softmax:0\", shape=(?, 50), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 19, 50)            1874200   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 19, 50)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)       (None, 1000)              3156000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                50050     \n",
      "=================================================================\n",
      "Total params: 5,080,250\n",
      "Trainable params: 5,080,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "\n",
    "# ToDo:\n",
    "# meterle self-attention (hay implementaciones en Keras)\n",
    "\n",
    "def custom_cosine_loss(emb):\n",
    "    # y_pred ya viene con embedding, y_true solo como one-hot\n",
    "    def fn(y_true, y_pred):\n",
    "        y_true_emb = emb.call(y_true)[0][0]\n",
    "        print(y_true)\n",
    "        print(y_true_emb)\n",
    "        print(y_pred)\n",
    "        #y_true_emb = np.array([y_true], dtype='int32')\n",
    "        #y_true_emb = tf.convert_to_tensor(y_true_emb)\n",
    "        #y_true_emb = model.layers[0].call(y_true)\n",
    "        #y_true_emb = K.get_value(y_true_emb)[0][0] # 50,\n",
    "        \n",
    "        return 1 - cosine_proximity(y_true_emb, y_pred)\n",
    "    return fn\n",
    "    \n",
    "emb_size = 50\n",
    "\n",
    "model = Sequential()\n",
    "emb = Embedding(n_items, emb_size, embeddings_initializer='uniform', input_length=19)\n",
    "model.add(emb)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(CuDNNGRU(1000)) \n",
    "model.add(Dropout(0.25))\n",
    "if embeddingp:\n",
    "    model.add(Dense(emb_size, activation='softmax'))\n",
    "    custom_loss = custom_cosine_loss(emb)  ## DUDA: Esta usando los pesos actuales?\n",
    "    model.compile(loss=custom_loss, optimizer='adam')\n",
    "else:\n",
    "    model.add(Dense(n_items, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "filepath=\"model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1i_adI_ASgDi"
   },
   "outputs": [],
   "source": [
    "train_fraction = 1#256 # 1/fraction es la cantidad de sesiones mas recientes a considerar\n",
    "dev_fraction = 1#2\n",
    "\n",
    "train_offset_step=40000#15530\n",
    "dev_offset_step=71#240\n",
    "\n",
    "\n",
    "aux = [0]\n",
    "aux.extend(list(train_data['ItemId'].unique()))\n",
    "itemids = np.array(aux)\n",
    "itemidmap = pd.Series(data=np.arange(n_items), index=itemids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "7FvXrjm3Sg1F",
    "outputId": "ae324c56-f702-4eb4-a79c-f79442617a02",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 2167s 54ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.00000, saving model to model.hdf5\n",
      "Epoch 1/1\n",
      "  513/40000 [..............................] - ETA: 1:26:18 - loss: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bfd414bc6e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                 \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_offset_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m#105,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                 callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1251\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2242\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2243\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1888\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#todo meterle un offset de sesiones al generador para poder continuar training al cargar pesos\n",
    "for epoch in range(1, 20):\n",
    "    train_generator = batch_generator(train_data, \n",
    "                                      batch_size=batch_size, \n",
    "                                      fraction=train_fraction, \n",
    "                                      offset=train_offset_step*epoch,\n",
    "                                     embedding=embeddingp,\n",
    "                                      n_items=n_items,\n",
    "                                     itemids=itemids,\n",
    "                                     itemidmap=itemidmap)\n",
    "    \n",
    "    dev_generator = batch_generator(dev_data, \n",
    "                                    batch_size=batch_size, \n",
    "                                    fraction=dev_fraction, \n",
    "                                    offset=dev_offset_step*epoch,\n",
    "                                    embedding=embeddingp,\n",
    "                                    n_items=n_items,\n",
    "                                    itemids=itemids,\n",
    "                                     itemidmap=itemidmap)\n",
    "    \n",
    "    history = model.fit_generator(train_generator,\n",
    "                                steps_per_epoch=train_offset_step,#15530,\n",
    "                                epochs=1,\n",
    "                                validation_data=dev_generator,\n",
    "                                validation_steps=dev_offset_step,#105,\n",
    "                                callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SL6iBzAvqJjF",
    "outputId": "8faaa34f-4323-4897-aaa1-cc8a0a2916e7"
   },
   "outputs": [],
   "source": [
    "# Test performance on test set\n",
    "\n",
    "test_generator = batch_generator(test_data, batch_size=batch_size)\n",
    "#model.load_weights('./drive/My Drive/Cursos/2018/IIC3633/model_8.h5')\n",
    "model.evaluate_generator(test_generator, steps=400, max_queue_size=10, workers=1, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "koA2hUR6PR_Q",
    "outputId": "6e75b798-1630-4a0d-aa40-7d2604c4f11c"
   },
   "outputs": [],
   "source": [
    "# Obtencion de metricas\n",
    "\n",
    "# Paso 1: Tomar el train set, y para cada ItemId sacar su one hot y luego su embedding. Guardar esto en una matriz\n",
    "# CONCLUSION: Esto ya está tal cual en la matriz de pesos de embedding. Para sacar el de un item, basta encontrar su itemidmap y luego comparar con la columna respectiva en ella\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "print(weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAnoGlasPclK"
   },
   "outputs": [],
   "source": [
    "# Paso 2: Dado un embedding de output desde el modelo, obtener los k=20 vectores mas cercanos en distancia sobre el espacio de embedding\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=20, algorithm='ball_tree').fit(weights)\n",
    "distances, indices = nbrs.kneighbors(weights) # Vienen ya ordenados! # Shape (37484, 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1476
    },
    "colab_type": "code",
    "id": "VNvk1oYaZIU1",
    "outputId": "86934cc8-3ae2-4583-e03d-2de17eada7f1"
   },
   "outputs": [],
   "source": [
    "# Paso 3: Dado un vector embedding arbitrario, obtener el item más cercano a éste. Aplicarla sobre los 20 anteriores.\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "test_generator = batch_generator(test_data, batch_size=batch_size)\n",
    "n = 0\n",
    "suma = 0\n",
    "while True:\n",
    "  test_batch = next(test_generator)\n",
    "  pred = model.predict(test_batch[0]) # 128, 50\n",
    "  label = test_batch[1]               # 128, 1\n",
    "\n",
    "\n",
    "  for row_idx in range(test_batch[0].shape[0]):\n",
    "    pred_row = pred[row_idx] # 50,\n",
    "    label_row = label[row_idx] # 50,\n",
    "\n",
    "    # embedding a label\n",
    "    elt = np.array([label_row], dtype='int32')\n",
    "    elt = tf.convert_to_tensor(elt)\n",
    "    called = model.layers[0].call(elt)\n",
    "    print(called.shape)\n",
    "    emb_label = K.get_value(called)[0][0] # 50,\n",
    "\n",
    "    # ahora, comparamos distancias\n",
    "    label_distances, label_indices = nbrs.kneighbors(emb_label.reshape(1, -1))\n",
    "    pred_distances, pred_indices = nbrs.kneighbors(pred_row.reshape(1, -1))\n",
    "\n",
    "\n",
    "    # OJO: Verificar que no ocurra que uno este sobre itemidmap y el otro sobre el rango normal\n",
    "    #print(label_distances)\n",
    "    #print(pred_distances)\n",
    "    print(label_indices)\n",
    "    print(pred_indices)\n",
    "    recall = recall_score(label_indices[0], pred_indices[0], average='macro')\n",
    "    print(recall)\n",
    "    suma += recall\n",
    "    n+=1\n",
    "    \n",
    "print(suma/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754735
    },
    "colab_type": "code",
    "id": "7wRVJm-LQs4s",
    "outputId": "dade5668-8df2-47b2-cfd7-7efd1f3781fc"
   },
   "outputs": [],
   "source": [
    "# Pasar params a fn\n",
    "\n",
    "def test2(data):\n",
    "  item_key = 'ItemId'\n",
    "  session_key = 'SessionId'\n",
    "  time_key = 'Time'\n",
    "\n",
    "  itemids = data[item_key].unique()\n",
    "  n_items = len(itemids)\n",
    "\n",
    "  itemidmap = pd.Series(data=np.arange(n_items), index=itemids) # Mapeo desde los 37.5k a (0, 37.5k) id\n",
    "  data = pd.merge(data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\n",
    "\n",
    "  for elt in indices[0]:\n",
    "    print()\n",
    "    \n",
    "  for dist in distances:\n",
    "    print(dist)\n",
    "    \n",
    "test2(train_data)\n",
    "\n",
    "# Paso 4: Ya tenemos toda la informacion: el output y los 20 más cercanos a éste\n",
    "# Paso 5: Calcular recall y MRR con librerías de manera sencilla (sklearn ofrece una, creo)\n",
    "\n",
    "# LUEGO DE ESTO\n",
    "# Si da muy mal comparado a M4 del paper, probar con 1000 hidden units.\n",
    "# Si sigue mal, entonces entrenar el v1 por mucho tiempo, copiar los pesos de esa embedding, pegarlos aca, y entrenar de nuevo\n",
    "# Si sigue mal, asumir pérdida por diferencia de implementación, y pasar a probar mecanismos de atención\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2472
    },
    "colab_type": "code",
    "id": "efc6brm5Hfqb",
    "outputId": "e4d83940-e8c0-4f98-ed28-d4c04fb11d53"
   },
   "outputs": [],
   "source": [
    "# Chequeo veracidad paso 1\n",
    "\n",
    "def test(train_data):\n",
    "  item_key = 'ItemId'\n",
    "  session_key = 'SessionId'\n",
    "  time_key = 'Time'\n",
    "\n",
    "  itemids = train_data[item_key].unique()\n",
    "  n_items = len(itemids)\n",
    "\n",
    "  itemidmap = pd.Series(data=np.arange(n_items), index=itemids) # Mapeo desde los 37.5k a (0, 37.5k) id\n",
    "  train_data = pd.merge(train_data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\n",
    "\n",
    "  for iii in range(15):\n",
    "    feats = np.array([train_data['ItemIdx'].unique()[iii]], dtype='int32')\n",
    "    print(feats)\n",
    "    if feats.shape[0] > session_max_len:\n",
    "        feats = feats[:session_max_len]\n",
    "    else:\n",
    "        feats = np.append(np.zeros((session_max_len-feats.shape[0],1), dtype=np.int8), feats) # left pad with zeros\n",
    "    print(feats)\n",
    "    feats = tf.convert_to_tensor(feats)\n",
    "    print(feats)\n",
    "    print(feats.shape)\n",
    "    emb_elt = K.get_value(model.layers[0].call(feats))\n",
    "    print(emb_elt[-1]==weights[0][iii])\n",
    "  \n",
    "test(train_data)\n",
    "\n",
    "def get_train_embs(train_data, model, emb_size):\n",
    "  out = np.zeros((n_items, emb_size))\n",
    "  idx = 0\n",
    "  #for name, values in train_data.iteritems():\n",
    "  #  if name=='ItemId':\n",
    "  #for elt in values:\n",
    "  for elt_idx in range(len(train_data['ItemId'].unique())):\n",
    "    if elt_idx % 1000 == 0:\n",
    "      print(elt_idx)\n",
    "    elt = np.array([train_data['ItemId'].unique()[elt_idx]], dtype='int32')\n",
    "    elt = tf.convert_to_tensor(elt)\n",
    "    emb_elt = K.get_value(model.layers[0].call(elt))\n",
    "    print(emb_elt)\n",
    "    out[idx, :] = emb_elt\n",
    "    idx += 1\n",
    "  print(out.shape)\n",
    "  return out\n",
    "\n",
    "emb_items = get_train_embs(train_data, model, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Az3Qd2UHMghX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IIC3633 Session RNNs v2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
