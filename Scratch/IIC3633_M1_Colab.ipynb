{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IIC3633_Session_RNNs_v2d.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "bhMT2zYloMpm"
      },
      "cell_type": "markdown",
      "source": [
        "#**Proyecto - Sistemas Recomendadores - IIC3633**\n",
        "\n",
        "## Implementación en Keras de Session-Based RNNs for Recommendation con soft atenttion\n",
        "\n",
        "### V2: Implementación de embedding sobre one-hot vectors para entrenamiento más eficiente y modelo más chico\n",
        "\n",
        "\n",
        "Preliminar: Configuración entorno GPUs, Google Drive, entre otros."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Z5dA3-L0nfLG",
        "outputId": "44d7f6d7-28ed-419e-ffe3-c29c417d129e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Manejo de Google Drive\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from google.colab import drive, auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6LGXgPsyoWsr",
        "outputId": "68f88bf8-43b9-4f28-8b80-554ec472daae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Librerias varias\n",
        "\n",
        "!pip install gputil\n",
        "!pip install humanize"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.6)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3kJRW-qQ17_Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "beb279fa-44e2-4607-c75b-2b2e810943b7"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import psutil\n",
        "import humanize\n",
        "import GPUtil as GPU\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers import Input, Dense, Dropout, CuDNNGRU, Embedding, concatenate, Lambda, multiply\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ik4GlGz8oZvj",
        "outputId": "f93d9202-dbcc-4a40-e634-cd5626383db1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "# Configuracion GPUs\n",
        "#!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "gpu = GPUs[0]\n",
        "\n",
        "def print_gpu_info():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize(\n",
        "          psutil.virtual_memory().available), \" I Proc size: \"  +\n",
        "          humanize.naturalsize(process.memory_info().rss))\n",
        "  print(\"GPU RAM Free {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total \\\n",
        "         {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, \n",
        "                           gpu.memoryTotal))\n",
        "  \n",
        "print_gpu_info()\n",
        "\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [(x.name, x.DESCRIPTOR, x.DEVICE_TYPE_FIELD_NUMBER, x.NAME_FIELD_NUMBER, x.PHYSICAL_DEVICE_DESC_FIELD_NUMBER) for x in local_device_protos if x.device_type == 'GPU']\n",
        "\n",
        "get_available_gpus()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 9.7 GB  I Proc size: 286.0 MB\n",
            "GPU RAM Free 10215MB | Used: 1226MB | Util  11% | Total          11441MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('/device:GPU:0',\n",
              "  <google.protobuf.pyext._message.MessageDescriptor at 0x7f9af1b4c870>,\n",
              "  2,\n",
              "  1,\n",
              "  7)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nfW00EfwSNQ6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cargamos dataframes preprocesados de RSC15\n",
        "PATH_TO_TRAIN = './drive/My Drive/Cursos/2018/IIC3633/processedData/rsc15_train_tr.txt'\n",
        "PATH_TO_DEV = './drive/My Drive/Cursos/2018/IIC3633/processedData/rsc15_train_valid.txt'\n",
        "PATH_TO_TEST = './drive/My Drive/Cursos/2018/IIC3633/processedData/rsc15_test.txt'\n",
        "\n",
        "train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
        "dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'ItemId':np.int64})\n",
        "test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YFkivhjnvrKO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_generator(data, batch_size=128, session_max_len=19, fraction=1, offset=0, embedding=True, n_items=None, itemids=None, itemidmap=None):\n",
        "    item_key = 'ItemId'\n",
        "    session_key = 'SessionId'\n",
        "    time_key = 'Time'\n",
        "    \n",
        "    data = pd.merge(data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\n",
        "    \n",
        "    #print(\"Cantidad de samples: {}\".format(len(data)//fraction))\n",
        "\n",
        "    data.sort_values([session_key, time_key], inplace=True) # ordenamos por sesion\n",
        "\n",
        "    #data.sort_values([time_key], inplace=True)\n",
        "    length = len(data['ItemId'])\n",
        "    #data = data[length-length//fraction:]\n",
        "    \n",
        "    offset_sessions = np.zeros(data[session_key].nunique()+1, dtype=np.int32)\n",
        "    offset_sessions[1:] = data.groupby(session_key).size().cumsum() # arreglo con offset acumulativo de inicio de cada sesion\n",
        "    #offset_sessions = offset_sessions[length-length//fraction:]\n",
        "    \n",
        "    actual_session = 0 + offset\n",
        "    \n",
        "    batch_feats = None\n",
        "    batch_labels = None\n",
        "    \n",
        "    # GRU_LAYER.reset_states() si usamos session parallel\n",
        "\n",
        "    while True:\n",
        "        datum = data[offset_sessions[actual_session]:offset_sessions[actual_session+1]][item_key]  # aqui toda la info de la sesion\n",
        "        datum = datum.values.reshape(-1,1)\n",
        "        \n",
        "        for i in range(offset_sessions[actual_session+1]-offset_sessions[actual_session]-1):\n",
        "            feats = datum[0:i+1]\n",
        "            if feats.shape[0] > session_max_len:\n",
        "                feats = feats[:session_max_len] # aca cambiar a mas nuevos\n",
        "            else:\n",
        "                feats = np.append(np.zeros((session_max_len-feats.shape[0],1), dtype=np.int8), feats) # left pad with zeros\n",
        "                      \n",
        "            feats = feats.reshape(1,-1) # (1, 19)\n",
        "                    \n",
        "            label = datum[i+1]\n",
        "            label = np.expand_dims(label, axis=0)  # Termina siendo (1, dimn_previa)\n",
        "            \n",
        "            if not isinstance(batch_feats, type(feats)):\n",
        "                batch_feats = feats\n",
        "            else:\n",
        "                batch_feats = np.append(batch_feats, feats, axis=0)\n",
        "\n",
        "            if not isinstance(batch_labels, type(label)):\n",
        "                batch_labels = label\n",
        "            else:\n",
        "                batch_labels = np.append(batch_labels, label, axis=0)\n",
        "\n",
        "            if batch_labels.shape[0] == batch_size:\n",
        "                if not embedding:\n",
        "                    # batch_labels.shape = (batch_size, 1)\n",
        "                    #new_labels = np.zeros((batch_size, n_items))\n",
        "                    #new_labels[0][:] = to_categorical(itemidmap[label[0][0]], num_classes=n_items)\n",
        "                    batch_labels = to_categorical(itemidmap[batch_labels.flatten()], num_classes=n_items)\n",
        "                \n",
        "                #print(\"Yielding batch with shape {} train, {} target\".format(batch_feats.shape, batch_labels.shape))\n",
        "                yield batch_feats, batch_labels\n",
        "                \n",
        "                # resume batch generation\n",
        "                batch_feats = None\n",
        "                batch_labels = None\n",
        "            \n",
        "        # TODO: Dropout random como en el paper\n",
        "        \n",
        "        actual_session = (actual_session + 1) % len(offset_sessions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eQslL5CpRjdQ",
        "outputId": "28a3637f-f783-47b0-fc68-fa149483cba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 512 #como en el paper\n",
        "session_max_len = 19\n",
        "embeddingp=False\n",
        "\n",
        "n_items = len(train_data['ItemId'].unique())+1\n",
        "print(\"Items unicos training:\", n_items)\n",
        "\n",
        "dev_n_items = len(dev_data['ItemId'].unique())+1\n",
        "print(\"Items unicos dev:\", dev_n_items)\n",
        "\n",
        "test_n_items = len(test_data['ItemId'].unique())+1\n",
        "print(\"Items unicos testing:\", test_n_items)\n",
        "\n",
        "train_samples_qty = len(train_data['SessionId'].unique()) # cantidad sesiones no augmentadas de train\n",
        "print(\"Sesiones training:\", train_samples_qty)\n",
        "\n",
        "dev_samples_qty = len(dev_data['SessionId'].unique()) # cantidad sesiones no augmentadas de dev\n",
        "print(\"Sesiones validation:\",dev_samples_qty)\n",
        "\n",
        "test_samples_qty = len(test_data['SessionId'].unique()) # cantidad sesiones no augmentadas de test\n",
        "print(\"Sesiones testing:\", test_samples_qty)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Items unicos training: 37484\n",
            "Items unicos dev: 6360\n",
            "Items unicos testing: 6752\n",
            "Sesiones training: 7953885\n",
            "Sesiones validation: 12372\n",
            "Sesiones testing: 15324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AdM3tjmSbxjG",
        "outputId": "8b1d3c31-2f24-48a8-d5a1-3a863685e7e1",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "cell_type": "code",
      "source": [
        "# Modelo\n",
        "\n",
        "# ToDo:\n",
        "# meterle self-attention (hay implementaciones en Keras)\n",
        "\n",
        "def custom_cosine_loss(model):\n",
        "    emb = model.layers[1]\n",
        "    # y_pred ya viene con embedding, y_true solo como one-hot\n",
        "    def fn(y_true, y_pred):\n",
        "        y_true_emb = emb.call(y_true)\n",
        "    #y_true_emb = np.array([y_true], dtype='int32')\n",
        "    #y_true_emb = tf.convert_to_tensor(y_true_emb)\n",
        "    #y_true_emb = model.layers[0].call(y_true)\n",
        "    #y_true_emb = K.get_value(y_true_emb)[0][0] # 50,\n",
        "\n",
        "        return 1 - cosine_proximity(y_true_emb, y_pred)\n",
        "    return fn\n",
        "    \n",
        "emb_size = 50\n",
        "size = emb_size if embeddingp else n_items\n",
        "\n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "emb = Embedding(n_items, emb_size, embeddings_initializer='uniform', input_length=19)\n",
        "model.add(emb)\n",
        "model.add(Dropout(0.25))\n",
        "model.add(CuDNNGRU(1000)) \n",
        "model.add(Dropout(0.25))\n",
        "if embeddingp:\n",
        "    model.add(Dense(emb_size, activation='softmax'))\n",
        "    custom_loss = custom_cosine_loss(emb)  ## DUDA: Esta usando los pesos actuales?\n",
        "    model.compile(loss=custom_loss, optimizer='adam')\n",
        "else:\n",
        "    model.add(Dense(n_items, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()\n",
        "\"\"\"\n",
        "\n",
        "inputs = Input(shape=(19,))\n",
        "emb = Embedding(n_items, emb_size, embeddings_initializer='uniform', input_length=19)(inputs)\n",
        "drop1 = Dropout(0.25)(emb)\n",
        "gru = CuDNNGRU(1000)(drop1)\n",
        "drop2 = Dropout(0.25)(gru)\n",
        "predictions = Dense(size, activation='softmax')(drop2)\n",
        "model = Model(input=inputs, output=[predictions])\n",
        "#custom_loss = custom_cosine_loss(model)\n",
        "model.compile(loss=categorical_crossentropy, optimizer='adam')\n",
        "model.summary()\n",
        "\n",
        "filepath=\"model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 19)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 19, 50)            1874200   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 19, 50)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)       (None, 1000)              3156000   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 37484)             37521484  \n",
            "=================================================================\n",
            "Total params: 42,551,684\n",
            "Trainable params: 42,551,684\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1i_adI_ASgDi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_fraction = 1#256 # 1/fraction es la cantidad de sesiones mas recientes a considerar\n",
        "dev_fraction = 1#2\n",
        "\n",
        "train_offset_step=35000#40000#15530\n",
        "dev_offset_step=65#240\n",
        "\n",
        "\n",
        "aux = [0]\n",
        "aux.extend(list(train_data['ItemId'].unique()))\n",
        "itemids = np.array(aux)\n",
        "itemidmap = pd.Series(data=np.arange(n_items), index=itemids) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7FvXrjm3Sg1F",
        "outputId": "34ff3ab9-71b6-4c6d-bd92-7ab9ef761ed2",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "#todo meterle un offset de sesiones al generador para poder continuar training al cargar pesos\n",
        "for epoch in range(1):\n",
        "    train_generator = batch_generator(train_data, \n",
        "                                      batch_size=batch_size, \n",
        "                                      fraction=train_fraction, \n",
        "                                      offset=train_offset_step*epoch,\n",
        "                                     embedding=embeddingp,\n",
        "                                      n_items=n_items,\n",
        "                                     itemids=itemids,\n",
        "                                     itemidmap=itemidmap)\n",
        "    \n",
        "    dev_generator = batch_generator(dev_data, \n",
        "                                    batch_size=batch_size, \n",
        "                                    fraction=dev_fraction, \n",
        "                                    offset=dev_offset_step*epoch,\n",
        "                                    embedding=embeddingp,\n",
        "                                    n_items=n_items,\n",
        "                                    itemids=itemids,\n",
        "                                     itemidmap=itemidmap)\n",
        "    \n",
        "    history = model.fit_generator(train_generator,\n",
        "                                steps_per_epoch=train_offset_step,#15530,\n",
        "                                epochs=1,\n",
        "                                validation_data=dev_generator,\n",
        "                                validation_steps=dev_offset_step,#105,\n",
        "                                callbacks=callbacks_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "35000/35000 [==============================] - 9610s 275ms/step - loss: 8.5877 - val_loss: 13.8648\n",
            "\n",
            "Epoch 00001: loss improved from inf to 8.58765, saving model to model.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v72MydLNAxbZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Epoch 1/1\n",
        "35000/35000 [==============================] - 9610s 275ms/step - loss: 8.5877 - val_loss: 13.8648\n",
        "\n",
        "Epoch 00001: loss improved from inf to 8.58765, saving model to model.hdf5"
      ]
    },
    {
      "metadata": {
        "id": "kYmwxyf6ARw4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "99ed3a00-2807-49f8-cb52-586a6df119df"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model.hdf5')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 59274, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 721, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 800, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SL6iBzAvqJjF",
        "outputId": "79c46855-3403-4091-a3df-86d79077fe26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Test performance on test set\n",
        "\n",
        "\n",
        "model.load_weights('./model.hdf5')\n",
        "test_generator = batch_generator(test_data, \n",
        "                                      batch_size=batch_size, \n",
        "                                      fraction=train_fraction, \n",
        "                                      offset=0,\n",
        "                                     embedding=embeddingp,\n",
        "                                      n_items=n_items,\n",
        "                                     itemids=itemids,\n",
        "                                     itemidmap=itemidmap)\n",
        "model.evaluate_generator(test_generator, steps=50, max_queue_size=10, workers=1, use_multiprocessing=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.851749114990234"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "95GjsYjtFd_i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "13.851749114990234"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "koA2hUR6PR_Q",
        "outputId": "e2b44002-e596-49aa-8f4b-33e7b76e606b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Obtencion de metricas\n",
        "\n",
        "# Paso 1: Tomar el train set, y para cada ItemId sacar su one hot y luego su embedding. Guardar esto en una matriz\n",
        "# CONCLUSION: Esto ya está tal cual en la matriz de pesos de embedding. Para sacar el de un item, basta encontrar su itemidmap y luego comparar con la columna respectiva en ella\n",
        "weights = model.layers[1].get_weights()[0]\n",
        "print(weights.shape)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(37484, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gAnoGlasPclK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Paso 2: Dado un embedding de output desde el modelo, obtener los k=20 vectores mas cercanos en distancia sobre el espacio de embedding\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "nbrs = NearestNeighbors(n_neighbors=20, algorithm='ball_tree').fit(weights)\n",
        "distances, indices = nbrs.kneighbors(weights) # Vienen ya ordenados! # Shape (37484, 20)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VNvk1oYaZIU1",
        "outputId": "9bc93e67-b3a0-4a94-c21f-20087826b1dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Paso 3: Dado un vector embedding arbitrario, obtener el item más cercano a éste. Aplicarla sobre los 20 anteriores.\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "test_generator = batch_generator(test_data, \n",
        "                                  batch_size=batch_size, \n",
        "                                  fraction=train_fraction, \n",
        "                                  offset=0,\n",
        "                                 embedding=embeddingp,\n",
        "                                  n_items=n_items,\n",
        "                                 itemids=itemids,\n",
        "                                 itemidmap=itemidmap)\n",
        "\n",
        "\n",
        "n = 0\n",
        "suma = 0\n",
        "while True:\n",
        "  try:\n",
        "    test_batch = next(test_generator)\n",
        "    pred = model.predict(test_batch[0]) # batch_size, n_items => 512, 37484\n",
        "    label = test_batch[1]               \n",
        "\n",
        "    #print(pred.shape)\n",
        "    #print(label.shape) \n",
        "\n",
        "    for row_idx in range(test_batch[0].shape[0]):\n",
        "      pred_row = pred[row_idx] # 37484, #.reshape(1, -1) # 50,\n",
        "      label_row = label[row_idx]        #.reshape(1, -1) # 50,\n",
        "\n",
        "      #print(pred_row.shape)\n",
        "      #print(label_row.shape)\n",
        "\n",
        "      idx1 = pred_row.argsort()[-20:][::-1]\n",
        "      idx2 = label_row.argsort()[-1:][::-1]\n",
        "\n",
        "      n += 1\n",
        "      #print(idx1)\n",
        "      #print(idx2)\n",
        "      if idx2[0] in idx1:\n",
        "        suma += 1\n",
        "\n",
        "  except:\n",
        "    break\n",
        "print(\"Recall: \", suma/n)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall:  0.008403813073394495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7wRVJm-LQs4s",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Pasar params a fn\n",
        "\n",
        "def test2(data):\n",
        "  item_key = 'ItemId'\n",
        "  session_key = 'SessionId'\n",
        "  time_key = 'Time'\n",
        "\n",
        "  itemids = data[item_key].unique()\n",
        "  n_items = len(itemids)\n",
        "\n",
        "  itemidmap = pd.Series(data=np.arange(n_items), index=itemids) # Mapeo desde los 37.5k a (0, 37.5k) id\n",
        "  data = pd.merge(data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\n",
        "\n",
        "  for elt in indices[0]:\n",
        "    print()\n",
        "    \n",
        "  for dist in distances:\n",
        "    print(dist)\n",
        "    \n",
        "test2(train_data)\n",
        "\n",
        "# Paso 4: Ya tenemos toda la informacion: el output y los 20 más cercanos a éste\n",
        "# Paso 5: Calcular recall y MRR con librerías de manera sencilla (sklearn ofrece una, creo)\n",
        "\n",
        "# LUEGO DE ESTO\n",
        "# Si da muy mal comparado a M4 del paper, probar con 1000 hidden units.\n",
        "# Si sigue mal, entonces entrenar el v1 por mucho tiempo, copiar los pesos de esa embedding, pegarlos aca, y entrenar de nuevo\n",
        "# Si sigue mal, asumir pérdida por diferencia de implementación, y pasar a probar mecanismos de atención\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "efc6brm5Hfqb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Chequeo veracidad paso 1\n",
        "\n",
        "def test(train_data):\n",
        "  item_key = 'ItemId'\n",
        "  session_key = 'SessionId'\n",
        "  time_key = 'Time'\n",
        "\n",
        "  itemids = train_data[item_key].unique()\n",
        "  n_items = len(itemids)\n",
        "\n",
        "  itemidmap = pd.Series(data=np.arange(n_items), index=itemids) # Mapeo desde los 37.5k a (0, 37.5k) id\n",
        "  train_data = pd.merge(train_data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\n",
        "\n",
        "  for iii in range(15):\n",
        "    feats = np.array([train_data['ItemIdx'].unique()[iii]], dtype='int32')\n",
        "    print(feats)\n",
        "    if feats.shape[0] > session_max_len:\n",
        "        feats = feats[:session_max_len]\n",
        "    else:\n",
        "        feats = np.append(np.zeros((session_max_len-feats.shape[0],1), dtype=np.int8), feats) # left pad with zeros\n",
        "    print(feats)\n",
        "    feats = tf.convert_to_tensor(feats)\n",
        "    print(feats)\n",
        "    print(feats.shape)\n",
        "    emb_elt = K.get_value(model.layers[0].call(feats))\n",
        "    print(emb_elt[-1]==weights[0][iii])\n",
        "  \n",
        "test(train_data)\n",
        "\n",
        "def get_train_embs(train_data, model, emb_size):\n",
        "  out = np.zeros((n_items, emb_size))\n",
        "  idx = 0\n",
        "  #for name, values in train_data.iteritems():\n",
        "  #  if name=='ItemId':\n",
        "  #for elt in values:\n",
        "  for elt_idx in range(len(train_data['ItemId'].unique())):\n",
        "    if elt_idx % 1000 == 0:\n",
        "      print(elt_idx)\n",
        "    elt = np.array([train_data['ItemId'].unique()[elt_idx]], dtype='int32')\n",
        "    elt = tf.convert_to_tensor(elt)\n",
        "    emb_elt = K.get_value(model.layers[0].call(elt))\n",
        "    print(emb_elt)\n",
        "    out[idx, :] = emb_elt\n",
        "    idx += 1\n",
        "  print(out.shape)\n",
        "  return out\n",
        "\n",
        "emb_items = get_train_embs(train_data, model, emb_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Az3Qd2UHMghX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}