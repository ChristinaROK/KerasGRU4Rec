{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IIC3633_M4 .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bhMT2zYloMpm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Proyecto - Sistemas Recomendadores - IIC3633**\n",
        "\n",
        "## Implementación en Keras de Session-Based RNNs for Recommendation con soft atenttion\n",
        "\n",
        "### V2: Predicción sobre embedding para entrenamiento más eficiente y modelo más chico\n",
        "\n",
        "\n",
        "Preliminar: Configuración entorno GPUs, Google Drive, entre otros."
      ]
    },
    {
      "metadata": {
        "id": "Z5dA3-L0nfLG",
        "colab_type": "code",
        "outputId": "fb27f92f-9659-4704-d6f6-706009e5746b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "# Manejo de Google Drive\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from google.colab import drive, auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6LGXgPsyoWsr",
        "colab_type": "code",
        "outputId": "74a7beb1-4abb-41c0-cfb5-b3cbc564f7c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# Librerias varias\n",
        "\n",
        "!pip install gputil\n",
        "!pip install humanize"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.6)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3kJRW-qQ17_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "505d76e6-8127-4db4-e650-0f95a7a54c6e"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import psutil\n",
        "import humanize\n",
        "import GPUtil as GPU\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers import Input, Dense, Dropout, CuDNNGRU, Embedding, concatenate, Lambda, multiply\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ik4GlGz8oZvj",
        "colab_type": "code",
        "outputId": "35f6ed40-4d9b-45e1-b17c-5ff22c5ef884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "# Configuracion GPUs\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "gpu = GPUs[0]\n",
        "\n",
        "def print_gpu_info():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize(\n",
        "          psutil.virtual_memory().available), \" I Proc size: \"  +\n",
        "          humanize.naturalsize(process.memory_info().rss))\n",
        "  print(\"GPU RAM Free {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total \\\n",
        "         {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, \n",
        "                           gpu.memoryTotal))\n",
        "  \n",
        "print_gpu_info()\n",
        "\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [(x.name, x.DESCRIPTOR, x.DEVICE_TYPE_FIELD_NUMBER, x.NAME_FIELD_NUMBER, x.PHYSICAL_DEVICE_DESC_FIELD_NUMBER) for x in local_device_protos if x.device_type == 'GPU']\n",
        "\n",
        "get_available_gpus()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.9 GB  I Proc size: 282.8 MB\n",
            "GPU RAM Free 7127MB | Used: 4314MB | Util  38% | Total          11441MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('/device:GPU:0',\n",
              "  <google.protobuf.pyext._message.MessageDescriptor at 0x7fb0e8447a70>,\n",
              "  2,\n",
              "  1,\n",
              "  7)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "nfW00EfwSNQ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cargamos dataframes preprocesados de RSC15\n",
        "PATH_TO_TRAIN = './drive/My Drive/Cursos/2018/IIC3633/processedData/rsc15_train_tr.txt'\n",
        "PATH_TO_DEV = './drive/My Drive/Cursos/2018/IIC3633/processedData/rsc15_train_valid.txt'\n",
        "PATH_TO_TEST = './drive/My Drive/Cursos/2018/IIC3633/processedData/rsc15_test.txt'\n",
        "\n",
        "train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
        "dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'ItemId':np.int64})\n",
        "test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fST8jLQBq7bA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_generator(data, batch_size=128, session_max_len=19, fraction=1, offset=0, embedding=True, n_items=None, itemids=None, itemidmap=None):\n",
        "    \"\"\"\n",
        "    Generador de batches para RSC15.\n",
        "    Se utiliza para entrenar, validar y testear en demanda.\n",
        "        args:\n",
        "            data: DataFrame pandas con info. a procesar\n",
        "            batch_size: tamanho batch\n",
        "            session_max_len: largo fijo para sesiones. De ser mayor, se trunca. De ser menor, left-zero-padding\n",
        "            fraction: fraccion mas reciente de sesiones que se consideran para la generacion\n",
        "            offset: parametro que permite salto manual de sesiones\n",
        "            embedding: Booleano que indica si el modelo actual predice embeddings, o one-hot encodings\n",
        "            n_items: cantidad de items unicos a predecir\n",
        "            itemids: lista con los IDs unicos de items en el set de datos a considerar\n",
        "            itemidmap: biyeccion de IDs en dataset a rango simple (0, n_items)\n",
        "    \"\"\"\n",
        "    \n",
        "    item_key = 'ItemId'\n",
        "    session_key = 'SessionId'\n",
        "    time_key = 'Time'\n",
        "    \n",
        "    # inner join dataframe con itemidmap\n",
        "    data = pd.merge(data, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner') # agrego esa columna\n",
        "    data.sort_values([session_key, time_key], inplace=True) # ordenamos por sesion\n",
        "\n",
        "    length = len(data['ItemId'])\n",
        "    #data.sort_values([time_key], inplace=True)\n",
        "    #data = data[length-length//fraction:]\n",
        "    \n",
        "    # arreglo con offset acumulativo de inicio de cada sesion\n",
        "    offset_sessions = np.zeros(data[session_key].nunique()+1, dtype=np.int32)\n",
        "    offset_sessions[1:] = data.groupby(session_key).size().cumsum() \n",
        "    #offset_sessions = offset_sessions[length-length//fraction:]\n",
        "    \n",
        "    actual_session = 0 + offset\n",
        "    batch_feats = None\n",
        "    batch_labels = None\n",
        "    \n",
        "    # K.reset_states(GRU_LAYER) si usamos session parallel approach\n",
        "\n",
        "    while True:\n",
        "        # la info de la sesion a considerar\n",
        "        datum = data[offset_sessions[actual_session]:offset_sessions[actual_session+1]][item_key]  \n",
        "        datum = datum.values.reshape(-1,1)\n",
        "        \n",
        "        # data augmentation\n",
        "        for i in range(offset_sessions[actual_session+1]-offset_sessions[actual_session]-1):\n",
        "            feats = datum[0:i+1]\n",
        "            \n",
        "            # truncate\n",
        "            if feats.shape[0] > session_max_len:\n",
        "                feats = feats[:session_max_len]\n",
        "            # zero padding\n",
        "            else:\n",
        "                feats = np.append(np.zeros((session_max_len-feats.shape[0],1), dtype=np.int8), feats) # left pad with zeros\n",
        "                      \n",
        "            feats = feats.reshape(1,-1) # (1, 19)        \n",
        "            label = datum[i+1]\n",
        "            label = np.expand_dims(label, axis=0)\n",
        "            \n",
        "            # add to batch\n",
        "            if not isinstance(batch_feats, type(feats)):\n",
        "                batch_feats = feats\n",
        "            else:\n",
        "                batch_feats = np.append(batch_feats, feats, axis=0)\n",
        "\n",
        "            if not isinstance(batch_labels, type(label)):\n",
        "                batch_labels = label\n",
        "            else:\n",
        "                batch_labels = np.append(batch_labels, label, axis=0)\n",
        "\n",
        "            # if batch is ready\n",
        "            if batch_labels.shape[0] == batch_size:\n",
        "                if not embedding:\n",
        "                    # do one hot if necessary\n",
        "                    batch_labels = to_categorical(itemidmap[batch_labels.flatten()], num_classes=n_items)\n",
        "                \n",
        "                # return batch\n",
        "                yield batch_feats, batch_labels\n",
        "                \n",
        "                # resume generation\n",
        "                batch_feats = None\n",
        "                batch_labels = None\n",
        "        \n",
        "        actual_session = (actual_session + 1) % len(offset_sessions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQslL5CpRjdQ",
        "colab_type": "code",
        "outputId": "80b0bea4-e1dc-464f-9ea4-65b3ddbe3c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "session_max_len = 19\n",
        "\n",
        "embeddingp=True\n",
        "\n",
        "n_items = len(train_data['ItemId'].unique())+1\n",
        "print(\"Items unicos training:\", n_items)\n",
        "\n",
        "dev_n_items = len(dev_data['ItemId'].unique())+1\n",
        "print(\"Items unicos dev:\", dev_n_items)\n",
        "\n",
        "test_n_items = len(test_data['ItemId'].unique())+1\n",
        "print(\"Items unicos testing:\", test_n_items)\n",
        "\n",
        "train_samples_qty = len(train_data['SessionId'].unique()) # cantidad sesiones no augmentadas de train\n",
        "print(\"Sesiones training:\", train_samples_qty)\n",
        "\n",
        "dev_samples_qty = len(dev_data['SessionId'].unique()) # cantidad sesiones no augmentadas de dev\n",
        "print(\"Sesiones validation:\",dev_samples_qty)\n",
        "\n",
        "test_samples_qty = len(test_data['SessionId'].unique()) # cantidad sesiones no augmentadas de test\n",
        "print(\"Sesiones testing:\", test_samples_qty)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Items unicos training: 37484\n",
            "Items unicos dev: 6360\n",
            "Items unicos testing: 6752\n",
            "Sesiones training: 7953885\n",
            "Sesiones validation: 12372\n",
            "Sesiones testing: 15324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AdM3tjmSbxjG",
        "colab_type": "code",
        "outputId": "d3042443-3525-479c-cac0-fd0a5494043e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "# Modelo\n",
        "\n",
        "import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.losses import cosine_proximity\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional, Dense, Dropout, CuDNNGRU, GRU, Embedding, Flatten, Input\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def custom_cosine_loss(emb):\n",
        "  \"\"\"Funcion custom para el modelo con embedding, m4\"\"\"\n",
        "    # y_pred: con embedding\n",
        "    # y_true: one-hot\n",
        "    def fn(y_true, y_pred):\n",
        "        # y_true_emb = emb.call(y_true)[0][0]\n",
        "        \n",
        "        #y_true_emb = np.array([y_true], dtype='int32')\n",
        "        #y_true_emb = tf.convert_to_tensor(y_true_emb)\n",
        "        y_true_emb = model.layers[0].call(y_true)\n",
        "        #y_true_emb = K.get_value(y_true_emb)[0][0] # 50,\n",
        "        \n",
        "        return 1 - cosine_proximity(y_true_emb, y_pred)\n",
        "    return fn\n",
        "    \n",
        "emb_size = 50\n",
        "    \n",
        "model = Sequential()\n",
        "emb = Embedding(n_items, emb_size, input_length=19)\n",
        "model.add(emb)\n",
        "model.add(Dropout(0.25))\n",
        "model.add(CuDNNGRU(1000)) \n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(emb_size, activation='softmax'))\n",
        "custom_loss = custom_cosine_loss(emb)\n",
        "model.compile(loss=custom_loss, optimizer='adam')\n",
        "model.summary()\n",
        "\n",
        "filepath=\"model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 19, 50)            1874200   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 19, 50)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)       (None, 1000)              3156000   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                50050     \n",
            "=================================================================\n",
            "Total params: 5,080,250\n",
            "Trainable params: 5,080,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1i_adI_ASgDi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_fraction = 1 # 1/fraction es la cantidad de sesiones mas recientes a considerar\n",
        "dev_fraction = 1\n",
        "\n",
        "train_offset_step=35000 #40000#15530\n",
        "dev_offset_step=65 #240\n",
        "\n",
        "\n",
        "aux = [0]\n",
        "aux.extend(list(train_data['ItemId'].unique()))\n",
        "itemids = np.array(aux)\n",
        "itemidmap = pd.Series(data=np.arange(n_items), index=itemids) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FvXrjm3Sg1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "    train_generator = batch_generator(train_data, \n",
        "                                      batch_size=batch_size, \n",
        "                                      fraction=train_fraction, \n",
        "                                      offset=train_offset_step*epoch,\n",
        "                                     embedding=embeddingp,\n",
        "                                      n_items=n_items,\n",
        "                                     itemids=itemids,\n",
        "                                     itemidmap=itemidmap)\n",
        "    \n",
        "    dev_generator = batch_generator(dev_data, \n",
        "                                    batch_size=batch_size, \n",
        "                                    fraction=dev_fraction, \n",
        "                                    offset=dev_offset_step*epoch,\n",
        "                                    embedding=embeddingp,\n",
        "                                    n_items=n_items,\n",
        "                                    itemids=itemids,\n",
        "                                     itemidmap=itemidmap)\n",
        "    \n",
        "    history = model.fit_generator(train_generator,\n",
        "                                steps_per_epoch=train_offset_step,#15530,\n",
        "                                epochs=1,\n",
        "                                validation_data=dev_generator,\n",
        "                                validation_steps=dev_offset_step,#105,\n",
        "                                callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b1mAJELcrVsS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "OLD RESULTS:\n",
        "\n",
        "Epoch 1/1\n",
        "Cantidad de samples: 123355\n",
        "239/240 [============================>.] - ETA: 5s - loss: 0.1683 Cantidad de samples: 29116\n",
        "240/240 [==============================] - 1455s 6s/step - loss: 0.1683 - val_loss: 0.2242\n",
        "\n",
        "Epoch 00001: loss improved from 0.52244 to 0.16832, saving model to model.hdf5\n",
        "Epoch 1/1\n",
        "Cantidad de samples: 123355\n",
        "239/240 [============================>.] - ETA: 5s - loss: 0.0711 Cantidad de samples: 29116\n",
        "240/240 [==============================] - 1440s 6s/step - loss: 0.0712 - val_loss: 0.1649\n",
        "\n",
        "Epoch 00001: loss improved from 0.16832 to 0.07116, saving model to model.hdf5\n",
        "Epoch 1/1\n",
        "Cantidad de samples: 123355\n",
        "239/240 [============================>.] - ETA: 6s - loss: 0.0317 Cantidad de samples: 29116\n",
        "240/240 [==============================] - 1469s 6s/step - loss: 0.0322 - val_loss: 0.1351\n",
        "\n",
        "Epoch 00001: loss improved from 0.07116 to 0.03215, saving model to model.hdf5\n",
        "Epoch 1/1\n",
        "Cantidad de samples: 123355"
      ]
    },
    {
      "metadata": {
        "id": "bmpJesGTHHLS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "ndrive = GoogleDrive(gauth)\n",
        "file = ndrive.CreateFile({'title': 'model.hdf5'})\n",
        "file.SetContentFile('./model.hdf5')\n",
        "file.Upload() \n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SL6iBzAvqJjF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Test performance on test set\n",
        "\n",
        "test_generator = batch_generator(test_data, batch_size=batch_size)\n",
        "model.load_weights('./drive/My Drive/Cursos/2018/IIC3633/model_8.h5')\n",
        "model.evaluate_generator(test_generator, steps=400, max_queue_size=10, workers=1, use_multiprocessing=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "koA2hUR6PR_Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Obtencion de metricas\n",
        "\n",
        "# Paso 1: Tomar el train set, y para cada ItemId sacar su one hot y luego su embedding. Guardar esto en una matriz\n",
        "# CONCLUSION: Esto ya está tal cual en la matriz de pesos de embedding. Para sacar el de un item, basta encontrar su itemidmap y luego comparar con la columna respectiva en ella\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "print(weights.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gAnoGlasPclK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Paso 2: Dado un embedding de output desde el modelo, obtener los k=20 vectores mas cercanos en distancia sobre el espacio de embedding\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "nbrs = NearestNeighbors(n_neighbors=20, algorithm='ball_tree').fit(weights)\n",
        "distances, indices = nbrs.kneighbors(weights) # Vienen ya ordenados! # Shape (37484, 20)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNvk1oYaZIU1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Paso 3: Dado un vector embedding arbitrario, obtener el item más cercano a éste. Aplicarla sobre los 20 anteriores.\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "test_generator = batch_generator(test_data, batch_size=batch_size)\n",
        "n = 0\n",
        "suma = 0\n",
        "while True:\n",
        "  test_batch = next(test_generator)\n",
        "  pred = model.predict(test_batch[0]) # 128, 50\n",
        "  label = test_batch[1]               # 128, 1\n",
        "\n",
        "\n",
        "  for row_idx in range(test_batch[0].shape[0]):\n",
        "    pred_row = pred[row_idx] # 50,\n",
        "    label_row = label[row_idx] # 50,\n",
        "\n",
        "    # embedding a label\n",
        "    elt = np.array([label_row], dtype='int32')\n",
        "    elt = tf.convert_to_tensor(elt)\n",
        "    called = model.layers[0].call(elt)\n",
        "    print(called.shape)\n",
        "    emb_label = K.get_value(called)[0][0] # 50,\n",
        "\n",
        "    # ahora, comparamos distancias\n",
        "    label_distances, label_indices = nbrs.kneighbors(emb_label.reshape(1, -1))\n",
        "    pred_distances, pred_indices = nbrs.kneighbors(pred_row.reshape(1, -1))\n",
        "\n",
        "\n",
        "    # OJO: Verificar que no ocurra que uno este sobre itemidmap y el otro sobre el rango normal\n",
        "    #print(label_distances)\n",
        "    #print(pred_distances)\n",
        "    print(label_indices)\n",
        "    print(pred_indices)\n",
        "    recall = recall_score(label_indices[0], pred_indices[0], average='macro')\n",
        "    print(recall)\n",
        "    suma += recall\n",
        "    n+=1\n",
        "    \n",
        "print(suma/n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7wRVJm-LQs4s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Paso 4: Ya tenemos toda la informacion: el output y los 20 más cercanos a éste\n",
        "# Paso 5: Calcular recall y MRR con librerías de manera sencilla (sklearn ofrece una, creo)\n",
        "\n",
        "# LUEGO DE ESTO\n",
        "# Si da muy mal comparado a M4 del paper, probar con 1000 hidden units.\n",
        "# Si sigue mal, entonces entrenar el v1 por mucho tiempo, copiar los pesos de esa embedding, pegarlos aca, y entrenar de nuevo\n",
        "# Si sigue mal, asumir pérdida por diferencia de implementación, y pasar a probar mecanismos de atención\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}