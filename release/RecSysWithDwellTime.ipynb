{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecSysWithDwellTime.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "bhMT2zYloMpm"
      },
      "cell_type": "markdown",
      "source": [
        "#**Proyecto - Sistemas Recomendadores - IIC3633**\n",
        "\n",
        "## Implementaci√≥n en Keras de Session-Based RNNs for Recommendation\n",
        "\n",
        "### Utilizacion de Dwell Time como feature implicita para mejor rendimiento"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3kJRW-qQ17_Q",
        "outputId": "1e336f48-43aa-4929-cefd-63e02dee3449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import psutil\n",
        "import humanize\n",
        "import GPUtil as GPU\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "import warnings\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers.core import Permute, Reshape, RepeatVector\n",
        "from keras.layers import Input, Dense, Dropout, CuDNNGRU, Embedding, concatenate, Lambda, multiply, merge, Flatten\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nfW00EfwSNQ6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cargamos dataframes preprocesados de MovieLens20MM\n",
        "PATH_TO_TRAIN = '../DwellTimeTheano/augmented.csv'\n",
        "PATH_TO_DEV = '../processedData/rsc15_train_valid.txt'\n",
        "PATH_TO_TEST = '../processedData/rsc15_test.txt'\n",
        "\n",
        "train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
        "dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'ItemId':np.int64})\n",
        "test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cO-7V2Koh_C6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SessionDataset:\n",
        "    def __init__(self, data, sep='\\t', session_key='SessionId', item_key='ItemId', time_key='Time', n_samples=-1, itemmap=None, time_sort=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path: path of the csv file\n",
        "            sep: separator for the csv\n",
        "            session_key, item_key, time_key: name of the fields corresponding to the sessions, items, time\n",
        "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
        "            itemmap: mapping between item IDs and item indices\n",
        "            time_sort: whether to sort the sessions by time or not\n",
        "        \"\"\"\n",
        "        self.df = data\n",
        "        self.session_key = session_key\n",
        "        self.item_key = item_key\n",
        "        self.time_key = time_key\n",
        "        self.time_sort = time_sort\n",
        "        self.add_item_indices(itemmap=itemmap)\n",
        "        self.df.sort_values([session_key, time_key], inplace=True)\n",
        "\n",
        "        #Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
        "        #clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
        "\n",
        "        self.click_offsets = self.get_click_offsets()\n",
        "        self.session_idx_arr = self.order_session_idx()\n",
        "        \n",
        "        \n",
        "    def get_click_offsets(self):\n",
        "        \"\"\"\n",
        "        Return the offsets of the beginning clicks of each session IDs,\n",
        "        where the offset is calculated against the first click of the first session ID.\n",
        "        \"\"\"\n",
        "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
        "        # group & sort the df by session_key and get the offset values\n",
        "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
        "\n",
        "        return offsets\n",
        "    \n",
        "\n",
        "    def order_session_idx(self):\n",
        "        \"\"\" Order the session indices \"\"\"\n",
        "        if self.time_sort:\n",
        "            # starting time for each sessions, sorted by session IDs\n",
        "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
        "            # order the session indices by session starting times\n",
        "            session_idx_arr = np.argsort(sessions_start_time)\n",
        "        else:\n",
        "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
        "\n",
        "        return session_idx_arr\n",
        "    \n",
        "    \n",
        "    def add_item_indices(self, itemmap=None):\n",
        "        \"\"\" \n",
        "        Add item index column named \"item_idx\" to the df\n",
        "        Args:\n",
        "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
        "        \"\"\"\n",
        "        if itemmap is None:\n",
        "            item_ids = self.df[self.item_key].unique()  # unique item ids\n",
        "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
        "                                 index=item_ids)\n",
        "            itemmap = pd.DataFrame({self.item_key:item_ids,\n",
        "                                   'item_idx':item2idx[item_ids].values})\n",
        "        \n",
        "        self.itemmap = itemmap\n",
        "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
        "        \n",
        "    \n",
        "    @property    \n",
        "    def items(self):\n",
        "        return self.itemmap.ItemId.unique()\n",
        "        \n",
        "\n",
        "class SessionDataLoader:\n",
        "    def __init__(self, dataset, batch_size=50):\n",
        "        \"\"\"\n",
        "        A class for creating session-parallel mini-batches.\n",
        "        Args:\n",
        "             dataset (SessionDataset): the session dataset to generate the batches from\n",
        "             batch_size (int): size of the batch\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.done_sessions_counter = 0\n",
        "        \n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
        "        Yields:\n",
        "            input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\n",
        "            target (B,): a Variable that stores the target item indices\n",
        "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
        "        \"\"\"\n",
        "\n",
        "        # initializations\n",
        "        df = self.dataset.df\n",
        "        session_key='SessionId'\n",
        "        item_key='ItemId'\n",
        "        time_key='TimeStamp'\n",
        "        self.n_items = df[item_key].nunique()+1\n",
        "        click_offsets = self.dataset.click_offsets\n",
        "        session_idx_arr = self.dataset.session_idx_arr\n",
        "\n",
        "        iters = np.arange(self.batch_size)\n",
        "        maxiter = iters.max()\n",
        "        start = click_offsets[session_idx_arr[iters]]\n",
        "        end = click_offsets[session_idx_arr[iters] + 1]\n",
        "        mask = [] # indicator for the sessions to be terminated\n",
        "        finished = False        \n",
        "\n",
        "        while not finished:\n",
        "            minlen = (end - start).min()\n",
        "            # Item indices(for embedding) for clicks where the first sessions start\n",
        "            idx_target = df.item_idx.values[start]\n",
        "            for i in range(minlen - 1):\n",
        "                # Build inputs & targets\n",
        "                idx_input = idx_target\n",
        "                idx_target = df.item_idx.values[start + i + 1]\n",
        "                input = idx_input\n",
        "                target = idx_target\n",
        "                yield input, target, mask\n",
        "                \n",
        "            # click indices where a particular session meets second-to-last element\n",
        "            start = start + (minlen - 1)\n",
        "            # see if how many sessions should terminate\n",
        "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
        "            self.done_sessions_counter = len(mask)\n",
        "            for idx in mask:\n",
        "                maxiter += 1\n",
        "                if maxiter >= len(click_offsets) - 1:\n",
        "                    finished = True\n",
        "                    break\n",
        "                # update the next starting/ending point\n",
        "                iters[idx] = maxiter\n",
        "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
        "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eQslL5CpRjdQ",
        "outputId": "bd8d7ef2-638d-42a1-eaf5-6e96636a7be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 512 #como en el paper\n",
        "session_max_len = 100\n",
        "embeddingp=False\n",
        "\n",
        "n_items = len(train_data['ItemId'].unique())+1\n",
        "print(\"Items unicos training:\", n_items)\n",
        "\n",
        "dev_n_items = len(dev_data['ItemId'].unique())+1\n",
        "print(\"Items unicos dev:\", dev_n_items)\n",
        "\n",
        "test_n_items = len(test_data['ItemId'].unique())+1\n",
        "print(\"Items unicos testing:\", test_n_items)\n",
        "\n",
        "train_samples_qty = len(train_data['SessionId'].unique()) # cantidad sesiones no augmentadas de train\n",
        "print(\"Sesiones training:\", train_samples_qty)\n",
        "\n",
        "dev_samples_qty = len(dev_data['SessionId'].unique()) # cantidad sesiones no augmentadas de dev\n",
        "print(\"Sesiones validation:\",dev_samples_qty)\n",
        "\n",
        "test_samples_qty = len(test_data['SessionId'].unique()) # cantidad sesiones no augmentadas de test\n",
        "print(\"Sesiones testing:\", test_samples_qty)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Items unicos training: 37484\n",
            "Items unicos dev: 6360\n",
            "Items unicos testing: 6752\n",
            "Sesiones training: 7953885\n",
            "Sesiones validation: 12372\n",
            "Sesiones testing: 15324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1i_adI_ASgDi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_fraction = 1#256 # 1/fraction es la cantidad de sesiones mas recientes a considerar\n",
        "dev_fraction = 1#2\n",
        "\n",
        "train_offset_step=train_samples_qty//batch_size\n",
        "dev_offset_step=dev_samples_qty//batch_size\n",
        "test_offset_step=test_samples_qty//batch_size\n",
        "\n",
        "\n",
        "aux = [0]\n",
        "aux.extend(list(train_data['ItemId'].unique()))\n",
        "itemids = np.array(aux)\n",
        "itemidmap = pd.Series(data=np.arange(n_items), index=itemids) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "O5_sa72xSF50",
        "outputId": "b1a69ec9-6e86-44ba-d19e-69fe1f59cf57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "cell_type": "code",
      "source": [
        "# Modelo\n",
        "   \n",
        "emb_size = 50\n",
        "hidden_units = 100\n",
        "size = emb_size\n",
        "\n",
        "inputs = Input(batch_shape=(batch_size, 1, n_items))\n",
        "gru, gru_states = CuDNNGRU(hidden_units, stateful=True, return_state=True)(inputs)# drop1) #\n",
        "drop2 = Dropout(0.25)(gru)\n",
        "predictions = Dense(n_items, activation='softmax')(drop2)\n",
        "model = Model(input=inputs, output=[predictions])\n",
        "# lr original es 0.0001\n",
        "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt)\n",
        "model.summary()\n",
        "\n",
        "filepath='./DwellTimeModel_checkpoint.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
        "callbacks_list = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (512, 1, 37484)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)       [(512, 100), (512, 100)]  11275800  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (512, 100)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (512, 37484)              3785884   \n",
            "=================================================================\n",
            "Total params: 15,061,684\n",
            "Trainable params: 15,061,684\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/pcerdam/.virtualenvs/KerasRecSysPy3/lib/python3.5/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rJyaJdgsh_DO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_states(model):\n",
        "    return [K.get_value(s) for s,_ in model.state_updates]\n",
        "\n",
        "def set_states(model, states):\n",
        "    for (d,_), s in zip(model.state_updates, states):\n",
        "        K.set_value(d, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p2wNiADth_DU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "def get_recall(model, train_generator_map, recall_k=20):\n",
        "\n",
        "\n",
        "    test_dataset = SessionDataset(test_data, itemmap=train_generator_map)\n",
        "    test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    n = 0\n",
        "    suma = 0\n",
        "    suma_baseline = 0\n",
        "\n",
        "    for feat, label, mask in test_generator:\n",
        "\n",
        "        \n",
        "        print(feat)\n",
        "        \n",
        "        input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
        "        input_oh = np.expand_dims(input_oh, axis=1)\n",
        "\n",
        "        target_oh = to_categorical(label, num_classes=loader.n_items)\n",
        "\n",
        "        pred = model.predict(input_oh, batch_size=batch_size)\n",
        "\n",
        "        if n%100 == 0:\n",
        "            try:\n",
        "                print(\"{}:{}\".format(n, suma/n))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        for row_idx in range(feat.shape[0]):\n",
        "            pred_row = pred[row_idx] \n",
        "            label_row = target_oh[row_idx]\n",
        "\n",
        "            idx1 = pred_row.argsort()[-recall_k:][::-1]\n",
        "            idx2 = label_row.argsort()[-1:][::-1]\n",
        "\n",
        "            n += 1\n",
        "            if idx2[0] in idx1:\n",
        "                suma += 1\n",
        "\n",
        "    print(\"Recall@{} epoch {}: {}\".format(recall_k, epoch, suma/n))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRUubaBRh_Db",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_mrr(model, train_generator_map, mrr_k=20):\n",
        "\n",
        "    test_dataset = SessionDataset(test_data, itemmap = train_generator_map)\n",
        "    test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    n = 0\n",
        "    suma = 0\n",
        "    suma_baseline = 0\n",
        "\n",
        "    for feat, label, mask in test_generator:\n",
        "        input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
        "        input_oh = np.expand_dims(input_oh, axis=1)\n",
        "        target_oh = to_categorical(label, num_classes=loader.n_items)\n",
        "\n",
        "        pred = model.predict(input_oh, batch_size=batch_size)\n",
        "\n",
        "        if n%100 == 0:\n",
        "            try:\n",
        "                print(\"{}:{}\".format(n, suma/n))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        for row_idx in range(feat.shape[0]):\n",
        "            pred_row = pred[row_idx] \n",
        "            label_row = target_oh[row_idx]\n",
        "\n",
        "            idx1 = pred_row.argsort()[-mrr_k:][::-1]\n",
        "            idx2 = label_row.argsort()[-1:][::-1]\n",
        "\n",
        "            n += 1\n",
        "            if idx2[0] in idx1:\n",
        "                suma += 1/int((np.where(idx1 == idx2[0])[0]+1))        \n",
        "\n",
        "    print(\"MRR@{} epoch {}: {}\".format(mrr_k, epoch, suma/n))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "F_IyYvRmh_Dh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = SessionDataset(train_data)\n",
        "\n",
        "model_to_train = model\n",
        "\n",
        "with tqdm(total=train_samples_qty) as pbar:\n",
        "    for epoch in range(3, 5):\n",
        "        loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
        "        for feat, target, mask in loader:\n",
        "\n",
        "            input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
        "            input_oh = np.expand_dims(input_oh, axis=1)\n",
        "\n",
        "            target_oh = to_categorical(target, num_classes=loader.n_items)\n",
        "\n",
        "            tr_loss = model_to_train.train_on_batch(input_oh, target_oh)\n",
        "\n",
        "            real_mask = np.ones((batch_size, 1))\n",
        "            for elt in mask:\n",
        "                real_mask[elt, :] = 0\n",
        "\n",
        "            hidden_states = get_states(model_to_train)[0]\n",
        "\n",
        "            hidden_states = np.multiply(real_mask, hidden_states)\n",
        "            hidden_states = np.array(hidden_states, dtype=np.float32)\n",
        "            model_to_train.layers[1].reset_states(hidden_states)\n",
        "            \n",
        "            pbar.set_description(\"Epoch {0}. Loss: {1:.5f}\".format(epoch, tr_loss))\n",
        "            pbar.update(loader.done_sessions_counter)\n",
        "\n",
        "        # get metrics for epoch\n",
        "        get_recall(model_to_train, train_dataset.itemmap)\n",
        "        get_mrr(model_to_train, train_dataset.itemmap)\n",
        "\n",
        "        # save model\n",
        "        model_to_train.save('./DwellTimeEpoch{}.h5'.format(epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}